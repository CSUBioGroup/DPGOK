{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "# from sklearn.metrics import average_precision_score as aupr\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict,deque,Counter\n",
    "import math\n",
    "import re\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='uniprot_API')\n",
    "    parser.add_argument('--predict')\n",
    "    parser.add_argument('--output_path')\n",
    "    parser.add_argument('--true')\n",
    "    parser.add_argument('--background')\n",
    "    parser.add_argument('--go')\n",
    "    parser.add_argument('--metrics')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "__all__ = ['fmax', 'aupr', 'ROOT_GO_TERMS', 'compute_performance', 'compute_performance_deepgoplus', 'read_pkl', 'save_pkl']\n",
    "ROOT_GO_TERMS = {'GO:0003674', 'GO:0008150', 'GO:0005575'}\n",
    "\n",
    "def fmax(go, targets, scores, idx_goid):\n",
    "    targets = ssp.csr_matrix(targets)\n",
    "    \n",
    "    # fmax_ = 0.0, 0.0, 0.0\n",
    "    fmax_ = 0.0, 0.0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    # icprecisions = []\n",
    "    # icrecalls = []\n",
    "    # dpprecisions = []\n",
    "    # dprecalls = []\n",
    "    # goic_list=[]\n",
    "    # godp_list=[]\n",
    "    # for i in range(len(idx_goid)):\n",
    "    #     goic_list.append(go.get_ic(idx_goid[i]))\n",
    "    # for i in range(len(idx_goid)):\n",
    "    #     godp_list.append(go.get_icdepth(idx_goid[i]))\n",
    "    # goic_vector=np.array(goic_list).reshape(-1,1)\n",
    "    # godp_vector=np.array(godp_list).reshape(-1,1)\n",
    "    \n",
    "    for cut in (c / 100 for c in range(101)):\n",
    "        cut_sc = ssp.csr_matrix((scores >= cut).astype(np.int32))\n",
    "        correct = cut_sc.multiply(targets).sum(axis=1)\n",
    "        correct_sc = cut_sc.multiply(targets)\n",
    "        fp_sc = cut_sc-correct_sc\n",
    "        fn_sc = targets-correct_sc\n",
    "        \n",
    "        # correct_ic=ssp.csr_matrix(correct_sc.dot(goic_vector))\n",
    "        # cut_ic=ssp.csr_matrix(cut_sc.dot(goic_vector))\n",
    "        # targets_ic=ssp.csr_matrix(targets.dot(goic_vector))\n",
    "        #\n",
    "        # correct_dp=ssp.csr_matrix(correct_sc.dot(godp_vector))\n",
    "        # cut_dp=ssp.csr_matrix(cut_sc.dot(godp_vector))\n",
    "        # targets_dp=ssp.csr_matrix(targets.dot(godp_vector))\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            p, r = correct / cut_sc.sum(axis=1), correct / targets.sum(axis=1)\n",
    "            p, r = np.average(p[np.invert(np.isnan(p))]), np.average(r)\n",
    "            \n",
    "            # mi=fp_sc.dot(goic_vector).sum(axis=0)\n",
    "            # ru=fn_sc.dot(goic_vector).sum(axis=0)\n",
    "            # mi/=len(targets.sum(axis=1))\n",
    "            # ru/=len(targets.sum(axis=1))\n",
    "            #\n",
    "            # icp, icr = correct_ic/cut_ic, correct_ic/targets_ic\n",
    "            # icp, icr = np.average(icp[np.invert(np.isnan(icp))]), np.average(icr)\n",
    "            #\n",
    "            # dpp, dpr = correct_dp/cut_dp, correct_dp/targets_dp\n",
    "            # dpp, dpr = np.average(dpp[np.invert(np.isnan(dpp))]), np.average(dpr)\n",
    "            \n",
    "        if np.isnan(p):\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(r)\n",
    "        else:\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "            \n",
    "        # if np.isnan(icp):\n",
    "        #     icprecisions.append(0.0)\n",
    "        #     icrecalls.append(icr)\n",
    "        # else:\n",
    "        #     icprecisions.append(icp)\n",
    "        #     icrecalls.append(icr)\n",
    "        #\n",
    "        # if np.isnan(dpp):\n",
    "        #     dpprecisions.append(0.0)\n",
    "        #     dprecalls.append(dpr)\n",
    "        # else:\n",
    "        #     dpprecisions.append(dpp)\n",
    "        #     dprecalls.append(dpr)\n",
    "        \n",
    "        try:\n",
    "            # fmax_ = max(fmax_, (2 * p * r / (p + r) if p + r > 0.0 else 0.0, math.sqrt(ru*ru + mi*mi) , cut))\n",
    "            fmax_ = max(fmax_, (2 * p * r / (p + r) if p + r > 0.0 else 0.0, cut))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "    # return fmax_[0], fmax_[1], fmax_[2], precisions, recalls, icprecisions, icrecalls, dpprecisions, dprecalls\n",
    "    return fmax_[0], fmax_[1], precisions, recalls\n",
    "\n",
    "def read_pkl(pklfile):\n",
    "    with open(pklfile,'rb') as fr:\n",
    "        data=pkl.load(fr)\n",
    "    return data\n",
    "\n",
    "def save_pkl(pklfile, data):\n",
    "    with open(pklfile,'wb') as fw:\n",
    "        pkl.dump(data, fw)\n",
    "\n",
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "FUNC_DICT = {\n",
    "    'cc': CELLULAR_COMPONENT,\n",
    "    'mf': MOLECULAR_FUNCTION,\n",
    "    'bp': BIOLOGICAL_PROCESS}\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}\n",
    "\n",
    "NAMESPACES_REVERT={\n",
    "    'cellular_component': 'cc',\n",
    "    'molecular_function': 'mf',\n",
    "    'biological_process': 'bp'\n",
    "}\n",
    "\n",
    "EXP_CODES = set(['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC',])\n",
    "CAFA_TARGETS = set([\n",
    "    '10090', '223283', '273057', '559292', '85962',\n",
    "    '10116',  '224308', '284812', '7227', '9606',\n",
    "    '160488', '237561', '321314', '7955', '99287',\n",
    "    '170187', '243232', '3702', '83333', '208963',\n",
    "    '243273', '44689', '8355'])\n",
    "\n",
    "def is_cafa_target(org):\n",
    "    return org in CAFA_TARGETS\n",
    "\n",
    "def is_exp_code(code):\n",
    "    return code in EXP_CODES\n",
    "\n",
    "class Ontology(object):\n",
    "    def __init__(self, filename, with_rels=False):\n",
    "        self.ont = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "        self.icdepth=None\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        self.icdepth={}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "            self.icdepth[go_id]=math.log(self.get_depth(go_id,NAMESPACES_REVERT[self.get_namespace(go_id)]),2)*self.ic[go_id]\n",
    "    \n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "    \n",
    "    def get_icdepth(self, go_id):\n",
    "        if self.icdepth is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.icdepth:\n",
    "            return 0.0\n",
    "        return self.icdepth[go_id]\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['is_a'].append(it[1]) \n",
    "                            \n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "        if obj is not None:\n",
    "            ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont\n",
    "\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while(len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "    \n",
    "    def get_depth(self,term_id,ont):\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        layer=1\n",
    "        while(len(q) > 0):\n",
    "            all_p=set()\n",
    "            while(len(q)>0):\n",
    "                t_id = q.popleft()\n",
    "                p_id=self.get_parents(t_id)\n",
    "                all_p.update(p_id)\n",
    "            if all_p:\n",
    "                layer+=1\n",
    "                for item in all_p:\n",
    "                    if item == FUNC_DICT[ont]:\n",
    "                        return layer\n",
    "                    q.append(item)\n",
    "        return layer\n",
    "\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "    \n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "        \n",
    "def new_compute_performance(test_df, go, ont):\n",
    "\n",
    "    go_set = go.get_namespace_terms(NAMESPACES[ont])\n",
    "    go_set.remove(FUNC_DICT[ont])\n",
    "    # print(len(go_set))\n",
    "    \n",
    "    labels = list(go_set)\n",
    "    goid_idx = {}\n",
    "    idx_goid = {}\n",
    "    for idx, goid in enumerate(labels):\n",
    "        goid_idx[goid] = idx\n",
    "        idx_goid[idx] = goid\n",
    "\n",
    "    pred_scores = []\n",
    "    true_scores = []\n",
    "    # Annotations\n",
    "    for i, row in enumerate(test_df.itertuples()):\n",
    "        # true\n",
    "        vals = [0]*len(labels)\n",
    "        annots = set()\n",
    "        for go_id in row.gos:\n",
    "            if go.has_term(go_id):\n",
    "                annots |= go.get_anchestors(go_id)\n",
    "        for go_id in annots:\n",
    "            if go_id in go_set:\n",
    "                vals[goid_idx[go_id]] = 1\n",
    "        true_scores.append(vals)\n",
    "\n",
    "        # pred\n",
    "        vals = [-1]*len(labels)\n",
    "        for items,score in row.predictions.items():\n",
    "#             print(f'item:{items},score:{score}, vals:{vals[goid_idx[items]]}')\n",
    "            score = float(score)\n",
    "            if items in go_set:\n",
    "                vals[goid_idx[items]] = max(score, vals[goid_idx[items]])\n",
    "            go_parent = go.get_anchestors(items)\n",
    "            for go_id in go_parent:\n",
    "                if go_id in go_set:\n",
    "                    vals[goid_idx[go_id]] = max(vals[goid_idx[go_id]], score)\n",
    "        pred_scores.append(vals)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    true_scores = np.array(true_scores)\n",
    "    # print(pred_scores.shape, true_scores.shape, sum(pred_scores<0), sum(pred_scores>0))\n",
    "    \n",
    "#     result_fmax, result_smin, result_t, precisions, recalls, icprecisions, icrecalls, dpprecisions, dprecalls = fmax(go, true_scores, pred_scores, idx_goid)\n",
    "    result_fmax, result_t, precisions, recalls = fmax(go, true_scores, pred_scores, idx_goid)\n",
    "    \n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    sorted_index = np.argsort(recalls)\n",
    "    recalls = recall[sorted_index]\n",
    "    precisions = precisions[sorted_index]\n",
    "    result_aupr = np.trapz(precisions, recalls)\n",
    "    \n",
    "#     icprecisions = np.array(icprecisions)\n",
    "#     icrecalls = np.array(icrecalls)\n",
    "#     sorted_index = np.argsort(icrecalls)\n",
    "#     icrecalls = icrecalls[sorted_index]\n",
    "#     icprecisions = icprecisions[sorted_index]\n",
    "#     result_icaupr = np.trapz(icprecisions, icrecalls)\n",
    "    \n",
    "#     dpprecisions = np.array(dpprecisions)\n",
    "#     dprecalls = np.array(dprecalls)\n",
    "#     sorted_index = np.argsort(dprecalls)\n",
    "#     dprecalls = dprecalls[sorted_index]\n",
    "#     dpprecisions = dpprecisions[sorted_index]\n",
    "#     result_dpaupr = np.trapz(dpprecisions, dprecalls)\n",
    "\n",
    "#     return result_fmax, result_smin , result_aupr, result_icaupr, result_dpaupr, result_t\n",
    "    return result_fmax, result_aupr, result_t\n",
    "\n",
    "\n",
    "# def generate_result(input_file, output_path, go_file, real_test_protein_mess, all_protein_information, metrics):\n",
    "#     all_files={}\n",
    "#     all_files['Your_method']=input_file\n",
    "    \n",
    "#     go = Ontology(go_file, with_rels=True)\n",
    "    \n",
    "#     all_annotations=[]\n",
    "#     for key,val in all_protein_information.items():\n",
    "#         item_set=set()\n",
    "#         for item in val['annotations']:\n",
    "#             item=item.split('|')[0]\n",
    "#             if go.has_term(item):\n",
    "#                 item_set |= go.get_anchestors(item)\n",
    "#         all_annotations.append(list(item_set))\n",
    "#     go.calculate_ic(all_annotations)\n",
    "\n",
    "#     all_tags = [ 'mf','cc','bp']\n",
    "#     all_results = OrderedDict()\n",
    "#     all_results['methods'] = []\n",
    "#     all_results['methods'].append(math.nan)\n",
    "#     for m in all_files.keys():\n",
    "#         all_results['methods'].append(m)\n",
    "#     all_metrics=['F_max', 'Smin', 'Aupr', 'ICAupr', 'DPAupr', 'threadhold']\n",
    "#     metric_list=[]\n",
    "#     for metric in metrics:\n",
    "#         metric_list.append(all_metrics[int(metric)])\n",
    "\n",
    "#     for evas in metric_list:\n",
    "#         for num, tag in enumerate(all_tags):\n",
    "#             all_results[\"{0}_{1}\".format(evas, num)] = []\n",
    "#             all_results[\"{0}_{1}\".format(evas, num)].append(tag)\n",
    "\n",
    "\n",
    "#     for num, tag in enumerate(all_tags):\n",
    "#         for method,mfile in all_files.items():\n",
    "#             save_dict = {}\n",
    "#             save_dict['protein_id'] = []\n",
    "#             save_dict['gos'] = []\n",
    "#             save_dict['predictions'] = []\n",
    "\n",
    "#             with open(mfile, 'rb') as fr:\n",
    "#                 method_predict_result = pkl.load(fr)\n",
    "\n",
    "#             for protein,val in method_predict_result.items():\n",
    "#                 if real_test_protein_mess[protein]['all_{0}'.format(tag)]==set():\n",
    "#                     continue\n",
    "#                 if tag not in method_predict_result[protein]:\n",
    "#                     method_predict_result[protein][tag]={}\n",
    "\n",
    "#                 save_dict['protein_id'].append(protein)\n",
    "#                 save_dict['gos'].append(real_test_protein_mess[protein]['all_{0}'.format(tag)])\n",
    "#                 save_dict['predictions'].append(method_predict_result[protein][tag])  #{go:score}\n",
    "\n",
    "#             df = pd.DataFrame(save_dict)\n",
    "#             F_max,Smin,Aupr,ICAupr, DPAupr,threadhold = new_compute_performance(df,go,tag)\n",
    "            \n",
    "#             if 'F_max' in metric_list:\n",
    "#                 all_results[\"{0}_{1}\".format('F_max', num)].append(round(F_max,5))\n",
    "#             if 'Smin' in metric_list:\n",
    "#                 all_results[\"{0}_{1}\".format('Smin', num)].append(round(Smin,5))\n",
    "#             if 'Aupr' in metric_list:\n",
    "#                 all_results[\"{0}_{1}\".format('Aupr', num)].append(round(Aupr,5))\n",
    "#             if 'ICAupr' in metric_list:\n",
    "#                 all_results[\"{0}_{1}\".format('ICAupr', num)].append(round(ICAupr,5))\n",
    "#             if 'DPAupr' in metric_list:\n",
    "#                 all_results[\"{0}_{1}\".format('DPAupr', num)].append(round(DPAupr,5))\n",
    "\n",
    "#             print('Have done', method, tag, 'F_max:', F_max, 'Smin:' ,Smin, 'Aupr:', Aupr,'ICAupr:', ICAupr,'DPAupr:', DPAupr,'threadhold:',threadhold)\n",
    "\n",
    "#     df = pd.DataFrame(all_results)\n",
    "#     df.to_csv('{0}/test_evaluation_results.csv'.format(output_path))\n",
    "    \n",
    "# def main(input_file, output_path, test_data_file, all_protein_information_file, go_file, metrics):\n",
    "#     with open(test_data_file,'rb') as f:\n",
    "#         test_data=pkl.load(f)\n",
    "    \n",
    "#     with open(all_protein_information_file,'rb') as f:\n",
    "#         all_protein_information=pkl.load(f)\n",
    "\n",
    "#     generate_result(input_file, output_path, go_file, test_data, all_protein_information, metrics)\n",
    "    \n",
    "# if __name__=='__main__':\n",
    "#     args=parse_args()\n",
    "#     input_file=args.predict\n",
    "#     output_path=args.output_path\n",
    "#     if not os.path.exists(output_path):\n",
    "#         os.mkdir(output_path)\n",
    "#     test_data_file=args.true\n",
    "#     all_protein_information_file=args.background\n",
    "#     go_file=args.go\n",
    "#     metrics=args.metrics\n",
    "#     metrics=metrics.strip().split(',')\n",
    "#     main(input_file, output_path, test_data_file, all_protein_information_file, go_file, metrics)\n",
    "\n",
    "def compute_performance_cafa(proteins, predscore, true_label, class_tag):\n",
    "    go_file = '/public/home/hpc244706074/myProject/data/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    file_path = f'/public/home/hpc244706074/myProject/CAFA/Train/{class_tag}_labels.csv'\n",
    "    # file_path = f'/public/home/hpc244706074/myProject/dataset/all_go_{class_tag}_terms.csv'\n",
    "    all_go = list(pd.read_csv(file_path)['functions'])\n",
    "    # with open(file_path,'rb') as f:\n",
    "    #     all_go = pickle.load(f)\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    true_gos = []\n",
    "    for label in true_label:\n",
    "        true_go = []\n",
    "        for i, l in enumerate(label):\n",
    "            if l == 1:\n",
    "                true_go.append(all_go[i])\n",
    "        true_gos.append(true_go)\n",
    "\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = true_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "    # F_max, Aupr, threadhold = new_compute_performance(df, go, all_go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, 'F_max:', F_max, 'Aupr:', Aupr, 'threadhold:', threadhold)\n",
    "\n",
    "    return F_max, Aupr, threadhold\n",
    "\n",
    "def compute_performance(proteins, predscore, true_label, class_tag):\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    file_path = f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{class_tag}_labels.csv'\n",
    "    all_go = list(pd.read_csv(file_path)['functions'])\n",
    "    # with open(file_path,'rb') as f:\n",
    "    #     all_go = pickle.load(f)\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    true_gos = []\n",
    "    for label in true_label:\n",
    "        true_go = []\n",
    "        for i, l in enumerate(label):\n",
    "            if l == 1:\n",
    "                true_go.append(all_go[i])\n",
    "        true_gos.append(true_go)\n",
    "\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = true_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, 'F_max:', F_max, 'Aupr:', Aupr, 'threadhold:', threadhold)\n",
    "\n",
    "    return F_max, Aupr, threadhold\n",
    "\n",
    "def compute_performance_test(proteins, predscore, true_gos, class_tag):\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    file_path = f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{class_tag}_labels.csv'\n",
    "    all_go = list(pd.read_csv(file_path)['functions'])\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    \n",
    "    all_gos = []\n",
    "    for p in proteins:\n",
    "        gos = [g for g in true_gos[p]]\n",
    "        all_gos.append(gos)\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = all_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    # F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, 'F_max:', F_max, 'Aupr:', Aupr, 'threadhold:', threadhold)\n",
    "\n",
    "    return F_max, Aupr, threadhold\n",
    "\n",
    "def compute_performance_gofreq(proteins, predscore, true_label, class_tag, all_go):\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    # file_path = f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{class_tag}_labels.csv'\n",
    "    # all_go = list(pd.read_csv(file_path)['functions'])\n",
    "    # with open(file_path,'rb') as f:\n",
    "    #     all_go = pickle.load(f)\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    true_gos = []\n",
    "    for label in true_label:\n",
    "        true_go = []\n",
    "        for i, l in enumerate(label):\n",
    "            if l == 1:\n",
    "                true_go.append(all_go[i])\n",
    "        true_gos.append(true_go)\n",
    "\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = true_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, 'F_max:', F_max, 'Aupr:', Aupr, 'threadhold:', threadhold)\n",
    "\n",
    "    return F_max, Aupr, threadhold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrays_to_dict(keys, values):\n",
    "    if len(keys) != len(values):\n",
    "        raise ValueError(\"数组的长度不一致\")\n",
    "    result = {}\n",
    "    for i in range(len(keys)):\n",
    "        result[keys[i]] = values[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_blast(ont,res_file):\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    with open(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl', 'rb') as fr:\n",
    "        p_labels = pkl.load(fr)\n",
    "        # labels = []\n",
    "        # for p in proteins:\n",
    "        #     list_l = [i for i in p_labels[p]]\n",
    "        #     labels.append(list_l)\n",
    "    \n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = float(s)\n",
    "        blast_r.append(r)\n",
    "\n",
    "    # with open(f'/public/home/hpc244706074/myProject/results/model_v32.1_{ont}_loss.pkl', 'rb') as fr:\n",
    "    with open(res_file, 'rb') as fr:\n",
    "        res = pkl.load(fr)\n",
    "\n",
    "    for i in range(0,11):\n",
    "        final_res = []\n",
    "        for j in range(0,len(proteins)):\n",
    "            r2 = np.array(res['preds'][j]) * (1-i/10) + np.array(blast_r[j]) * (i/10)\n",
    "            final_res.append(r2)\n",
    "        print(f'my:{(1-i/10)}; blast:{i/10}')\n",
    "        F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "        \n",
    "        print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "        \n",
    "def compute_performance_zero(proteins, predscore, true_gos, class_tag, all_go):\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    # file_path = f'/media/asus/data16t/yangqr/project/dataset/select_min_count_1_{class_tag}_labels.csv'\n",
    "    # file_path = f'/media/asus/data16t/yangqr/project/dataset/train&test_{class_tag}_labels.csv'\n",
    "    # all_go = list(pd.read_csv(file_path)['functions'])\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    \n",
    "    all_gos = []\n",
    "    for p in proteins:\n",
    "        gos = [g for g in true_gos[p]]\n",
    "        all_gos.append(gos)\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = all_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    # F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, 'F_max:', F_max, 'Aupr:', Aupr, 'threadhold:', threadhold)\n",
    "\n",
    "    return F_max, Aupr, threadhold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "my:1.0; blast:0.0\n",
      "Have done mf F_max: 0.6068502351744793 Aupr: 0.6136418842615894 threadhold: 0.5\n",
      "mf: aupr:0.613642,F_max:0.606850,threadhold:0.500000\n",
      "\n",
      "my:0.9; blast:0.1\n",
      "Have done mf F_max: 0.6086889447608227 Aupr: 0.5917582875121208 threadhold: 0.43\n",
      "mf: aupr:0.591758,F_max:0.608689,threadhold:0.430000\n",
      "\n",
      "my:0.8; blast:0.2\n",
      "Have done mf F_max: 0.6103971725725684 Aupr: 0.5952124182454275 threadhold: 0.44\n",
      "mf: aupr:0.595212,F_max:0.610397,threadhold:0.440000\n",
      "\n",
      "my:0.7; blast:0.3\n",
      "Have done mf F_max: 0.6105641855466061 Aupr: 0.5926666564865843 threadhold: 0.41\n",
      "mf: aupr:0.592667,F_max:0.610564,threadhold:0.410000\n",
      "\n",
      "my:0.6; blast:0.4\n",
      "Have done mf F_max: 0.6128228129388678 Aupr: 0.5911323631483557 threadhold: 0.32\n",
      "mf: aupr:0.591132,F_max:0.612823,threadhold:0.320000\n",
      "\n",
      "my:0.5; blast:0.5\n",
      "Have done mf F_max: 0.6176253001442866 Aupr: 0.5893265264696086 threadhold: 0.35\n",
      "mf: aupr:0.589327,F_max:0.617625,threadhold:0.350000\n",
      "\n",
      "my:0.4; blast:0.6\n",
      "Have done mf F_max: 0.6203075133158015 Aupr: 0.5850745985010912 threadhold: 0.31\n",
      "mf: aupr:0.585075,F_max:0.620308,threadhold:0.310000\n",
      "\n",
      "my:0.30000000000000004; blast:0.7\n",
      "Have done mf F_max: 0.6189361465401577 Aupr: 0.5796684882659063 threadhold: 0.23\n",
      "mf: aupr:0.579668,F_max:0.618936,threadhold:0.230000\n",
      "\n",
      "my:0.19999999999999996; blast:0.8\n",
      "Have done mf F_max: 0.6107246215844074 Aupr: 0.5723462799315404 threadhold: 0.19\n",
      "mf: aupr:0.572346,F_max:0.610725,threadhold:0.190000\n",
      "\n",
      "my:0.09999999999999998; blast:0.9\n",
      "Have done mf F_max: 0.5900986179076503 Aupr: 0.5578318046265671 threadhold: 0.28\n",
      "mf: aupr:0.557832,F_max:0.590099,threadhold:0.280000\n",
      "\n",
      "my:0.0; blast:1.0\n",
      "Have done mf F_max: 0.5866879975923937 Aupr: 0.4874389351351024 threadhold: 0.31\n",
      "mf: aupr:0.487439,F_max:0.586688,threadhold:0.310000\n",
      "\n",
      "cc\n",
      "my:1.0; blast:0.0\n",
      "Have done cc F_max: 0.6716643633457812 Aupr: 0.702808694780408 threadhold: 0.4\n",
      "mf: aupr:0.702809,F_max:0.671664,threadhold:0.400000\n",
      "\n",
      "my:0.9; blast:0.1\n",
      "Have done cc F_max: 0.6726038465578408 Aupr: 0.6491309948524014 threadhold: 0.48\n",
      "mf: aupr:0.649131,F_max:0.672604,threadhold:0.480000\n",
      "\n",
      "my:0.8; blast:0.2\n",
      "Have done cc F_max: 0.6744419821790992 Aupr: 0.6893096078379986 threadhold: 0.5\n",
      "mf: aupr:0.689310,F_max:0.674442,threadhold:0.500000\n",
      "\n",
      "my:0.7; blast:0.3\n",
      "Have done cc F_max: 0.675681645985652 Aupr: 0.6421070729619883 threadhold: 0.47\n",
      "mf: aupr:0.642107,F_max:0.675682,threadhold:0.470000\n",
      "\n",
      "my:0.6; blast:0.4\n",
      "Have done cc F_max: 0.6748792548336753 Aupr: 0.6797921590433685 threadhold: 0.42\n",
      "mf: aupr:0.679792,F_max:0.674879,threadhold:0.420000\n",
      "\n",
      "my:0.5; blast:0.5\n",
      "Have done cc F_max: 0.6746334477344671 Aupr: 0.6738583736349989 threadhold: 0.39\n",
      "mf: aupr:0.673858,F_max:0.674633,threadhold:0.390000\n",
      "\n",
      "my:0.4; blast:0.6\n",
      "Have done cc F_max: 0.6721647425455091 Aupr: 0.6665250361584979 threadhold: 0.34\n",
      "mf: aupr:0.666525,F_max:0.672165,threadhold:0.340000\n",
      "\n",
      "my:0.30000000000000004; blast:0.7\n",
      "Have done cc F_max: 0.6664929868277801 Aupr: 0.6555843649383757 threadhold: 0.26\n",
      "mf: aupr:0.655584,F_max:0.666493,threadhold:0.260000\n",
      "\n",
      "my:0.19999999999999996; blast:0.8\n",
      "Have done cc F_max: 0.6487400275115017 Aupr: 0.6385526099303453 threadhold: 0.19\n",
      "mf: aupr:0.638553,F_max:0.648740,threadhold:0.190000\n",
      "\n",
      "my:0.09999999999999998; blast:0.9\n",
      "Have done cc F_max: 0.6155718801728756 Aupr: 0.6160861404962611 threadhold: 0.09\n",
      "mf: aupr:0.616086,F_max:0.615572,threadhold:0.090000\n",
      "\n",
      "my:0.0; blast:1.0\n",
      "Have done cc F_max: 0.5787811947613057 Aupr: 0.4488601701925304 threadhold: 0.34\n",
      "mf: aupr:0.448860,F_max:0.578781,threadhold:0.340000\n",
      "\n",
      "bp\n",
      "my:1.0; blast:0.0\n",
      "Have done bp F_max: 0.42781600823197236 Aupr: 0.35603192529406513 threadhold: 0.39\n",
      "mf: aupr:0.356032,F_max:0.427816,threadhold:0.390000\n",
      "\n",
      "my:0.9; blast:0.1\n",
      "Have done bp F_max: 0.4293374629193104 Aupr: 0.37937473486247775 threadhold: 0.36\n",
      "mf: aupr:0.379375,F_max:0.429337,threadhold:0.360000\n",
      "\n",
      "my:0.8; blast:0.2\n",
      "Have done bp F_max: 0.4321912325042923 Aupr: 0.3829768033570045 threadhold: 0.38\n",
      "mf: aupr:0.382977,F_max:0.432191,threadhold:0.380000\n",
      "\n",
      "my:0.7; blast:0.3\n",
      "Have done bp F_max: 0.4373477470662086 Aupr: 0.3850995298506692 threadhold: 0.37\n",
      "mf: aupr:0.385100,F_max:0.437348,threadhold:0.370000\n",
      "\n",
      "my:0.6; blast:0.4\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf','cc', 'bp']:\n",
    "    print(ont)\n",
    "    res_file = f'/public/home/hpc244706074/myProject/results/model_v45_{ont}_fmax.pkl'\n",
    "    add_blast(ont,res_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ont in ['mf','cc']:\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    with open(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl', 'rb') as fr:\n",
    "        p_labels = pkl.load(fr)\n",
    "        labels = []\n",
    "        for p in proteins:\n",
    "            list_l = [i for i in p_labels[p]]\n",
    "            labels.append(list_l)\n",
    "    \n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = 1\n",
    "        blast_r.append(r)\n",
    "\n",
    "    with open(f'/public/home/hpc244706074/myProject/results/model_v32.1_{ont}_loss.pkl', 'rb') as fr:\n",
    "        res = pkl.load(fr)\n",
    "\n",
    "    for i in range(0,11):\n",
    "        final_res = []\n",
    "        for j in range(0,len(proteins)):\n",
    "            r2 = np.array(res['preds'][j]) * (1-i/10) + np.array(blast_r[j]) * (i/10)\n",
    "            final_res.append(r2)\n",
    "        print(f'my:{(1-i/10)}; blast:{i/10}')\n",
    "        F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, labels , ont)\n",
    "        \n",
    "        print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                        (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_deepgozero_mf_predict.pkl', 'rb') as fr:\n",
    "    res = pkl.load(fr)\n",
    "ont = 'mf'\n",
    "proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "\n",
    "with open(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl', 'rb') as fr:\n",
    "    p_labels = pkl.load(fr)\n",
    "    labels = []\n",
    "    for p in proteins:\n",
    "        list_l = [i for i in p_labels[p]]\n",
    "        labels.append(list_l)\n",
    "\n",
    "F_max, Aupr, threadhold = compute_performance_test(proteins, res, labels , ont)\n",
    "print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几个模型取均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_final_res(ont, matrix_list , method='mean'):\n",
    "    if method == 'mean':\n",
    "        res = np.mean(matrix_list, axis=0)\n",
    "    elif method == 'max':\n",
    "        res = np.max(matrix_list, axis=0)\n",
    "        \n",
    "    return res\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_n in ['final1', 'final1.1']:\n",
    "    print(model_n)\n",
    "    for c in [1,6,11]:\n",
    "        print(c)\n",
    "        for ont in ['mf','cc','bp']:\n",
    "            print(ont)\n",
    "            proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "            gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "            go_idx = {g:i for i,g in enumerate(gos)}\n",
    "            p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "            res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v{model_n}_{ont}_count_{c}.pkl')['preds']\n",
    "            res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v{model_n}_{ont}_count_{c+1}.pkl')['preds']\n",
    "            res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v{model_n}_{ont}_count_{c+2}.pkl')['preds']\n",
    "            res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v{model_n}_{ont}_count_{c+3}.pkl')['preds']\n",
    "            res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v{model_n}_{ont}_count_{c+4}.pkl')['preds']\n",
    "            matrix_list = [res1, res2, res3]\n",
    "            final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "            F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "            print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "            # final_res2 = get_final_res(ont, matrix_list , method='max')\n",
    "            # F_max, Aupr, threadhold = compute_performance_test(proteins, final_res2, p_labels , ont)\n",
    "            # print('mf max: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp\n",
      "Have done bp F_max: 0.438622452456176 Aupr: 0.387565851686478 threadhold: 0.38\n",
      "mf mean: aupr:0.387566,F_max:0.438622,threadhold:0.380000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['bp']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v100_{ont}_count_5.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v100_{ont}_count_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v100_{ont}_count_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v100_{ont}_count_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v100_{ont}_count_4.pkl')['preds']\n",
    "    matrix_list = [res1, res2, res3, res4, res5]\n",
    "    final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    # final_res2 = get_final_res(ont, matrix_list , method='max')\n",
    "    # F_max, Aupr, threadhold = compute_performance_test(proteins, final_res2, p_labels , ont)\n",
    "    # print('mf max: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "Have done mf F_max: 0.6123996717971768 Aupr: 0.5368723543109389 threadhold: 0.29\n",
      "mf mean: aupr:0.536872,F_max:0.612400,threadhold:0.290000\n",
      "\n",
      "Have done mf F_max: 0.6087530171016504 Aupr: 0.6059031132584404 threadhold: 0.54\n",
      "mf max: aupr:0.605903,F_max:0.608753,threadhold:0.540000\n",
      "\n",
      "cc\n",
      "Have done cc F_max: 0.6721136352629798 Aupr: 0.6268242660007269 threadhold: 0.5\n",
      "mf mean: aupr:0.626824,F_max:0.672114,threadhold:0.500000\n",
      "\n",
      "Have done cc F_max: 0.6698313183863961 Aupr: 0.6793121900544995 threadhold: 0.85\n",
      "mf max: aupr:0.679312,F_max:0.669831,threadhold:0.850000\n",
      "\n",
      "bp\n",
      "Have done bp F_max: 0.4450136357850027 Aupr: 0.3842443313993585 threadhold: 0.48\n",
      "mf mean: aupr:0.384244,F_max:0.445014,threadhold:0.480000\n",
      "\n",
      "Have done bp F_max: 0.4382905284449978 Aupr: 0.3221092380639904 threadhold: 0.78\n",
      "mf max: aupr:0.322109,F_max:0.438291,threadhold:0.780000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf','cc','bp']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_0.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_4.pkl')['preds']\n",
    "    matrix_list = [res1, res2, res3, res4, res5]\n",
    "    final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    final_res2 = get_final_res(ont, matrix_list , method='max')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res2, p_labels , ont)\n",
    "    print('mf max: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#先取均值再与blast加权\n",
    "def add_blast(ont,res):\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    with open(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl', 'rb') as fr:\n",
    "        p_labels = pkl.load(fr)\n",
    "        # labels = []\n",
    "        # for p in proteins:\n",
    "        #     list_l = [i for i in p_labels[p]]\n",
    "        #     labels.append(list_l)\n",
    "    \n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = float(s)\n",
    "        blast_r.append(r)\n",
    "\n",
    "    # with open(f'/public/home/hpc244706074/myProject/results/model_v32.1_{ont}_loss.pkl', 'rb') as fr:\n",
    "    # with open(res_file, 'rb') as fr:\n",
    "    #     res = pkl.load(fr)\n",
    "\n",
    "    for i in range(0,11):\n",
    "        final_res = []\n",
    "        for j in range(0,len(proteins)):\n",
    "            r2 = np.array(res[j]) * (1-i/10) + np.array(blast_r[j]) * (i/10)\n",
    "            final_res.append(r2)\n",
    "        print(f'my:{(1-i/10)}; blast:{i/10}')\n",
    "        F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "        \n",
    "        print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "        \n",
    "for ont in ['bp']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    # res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_5.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.3_{ont}_fmax_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.3_{ont}_fmax_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.3_{ont}_fmax_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.3_{ont}_fmax_4.pkl')['preds']\n",
    "    matrix_list = [ res2, res3, res4, res5]\n",
    "    final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    add_blast(ont, final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "my:0.9; blast:0.1\n",
      "Have done mf F_max: 0.6206417725189816 Aupr: 0.5973451049292027 threadhold: 0.51\n",
      "Have done mf F_max: 0.6086866450358559 Aupr: 0.5722604873567468 threadhold: 0.8\n",
      "my:0.8; blast:0.2\n",
      "Have done mf F_max: 0.6223165231967753 Aupr: 0.59831935794957 threadhold: 0.42\n",
      "Have done mf F_max: 0.6165746842555225 Aupr: 0.5745203788770145 threadhold: 0.65\n",
      "my:0.7; blast:0.3\n",
      "Have done mf F_max: 0.6290000570165615 Aupr: 0.5950900868304765 threadhold: 0.38\n",
      "Have done mf F_max: 0.6196864951054595 Aupr: 0.5710377894828266 threadhold: 0.47\n",
      "my:0.6; blast:0.4\n",
      "Have done mf F_max: 0.6250911909053553 Aupr: 0.5837743358256917 threadhold: 0.27\n",
      "Have done mf F_max: 0.6143899336026959 Aupr: 0.566030558516318 threadhold: 0.3\n",
      "my:0.5; blast:0.5\n",
      "Have done mf F_max: 0.5968871145143045 Aupr: 0.5658046538372185 threadhold: 0.14\n",
      "Have done mf F_max: 0.5937552433987197 Aupr: 0.5551438730125478 threadhold: 0.3\n",
      "my:0.4; blast:0.6\n",
      "Have done mf F_max: 0.5888306891599195 Aupr: 0.5516544885133472 threadhold: 0.3\n",
      "Have done mf F_max: 0.5902249071294594 Aupr: 0.5434618297738095 threadhold: 0.32\n",
      "my:0.30000000000000004; blast:0.7\n",
      "Have done mf F_max: 0.5879538936621005 Aupr: 0.5372296116140124 threadhold: 0.32\n",
      "Have done mf F_max: 0.5888503365082891 Aupr: 0.5346378838421543 threadhold: 0.32\n",
      "my:0.19999999999999996; blast:0.8\n",
      "Have done mf F_max: 0.5883451348973121 Aupr: 0.5175627203467422 threadhold: 0.31\n",
      "Have done mf F_max: 0.5881382780526957 Aupr: 0.5173925560548417 threadhold: 0.31\n",
      "my:0.09999999999999998; blast:0.9\n",
      "Have done mf F_max: 0.5866879975923937 Aupr: 0.51693379096325 threadhold: 0.31\n",
      "Have done mf F_max: 0.5866879975923937 Aupr: 0.5170058969123985 threadhold: 0.31\n"
     ]
    }
   ],
   "source": [
    "#先加权相加再取均值或最大值\n",
    "for ont in ['mf']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v00163.._{ont}_count_5.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v00163.._{ont}_count_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v00163.._{ont}_count_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v00163.._{ont}_count_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v00163.._{ont}_count_4.pkl')['preds']\n",
    "    \n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = float(s)\n",
    "        blast_r.append(r)\n",
    "    for i in range(1,10):\n",
    "        print(f'my:{(1-i/10)}; blast:{i/10}')\n",
    "        res1 = np.array(res1) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "        res2 = np.array(res2) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "        res3 = np.array(res3) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "        res4 = np.array(res4) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "        res5 = np.array(res5) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "\n",
    "        matrix_list = [res1, res2, res3, res4, res5]\n",
    "        final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "        # final_res0 = get_final_res(ont, matrix_list , method='max')\n",
    "        F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "        # F_max, Aupr, threadhold = compute_performance_test(proteins, final_res0, p_labels , ont)\n",
    "        # print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    # add_blast(ont, final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp\n",
      "Have done bp F_max: 0.44828308732476635 Aupr: 0.38276659165025184 threadhold: 0.32\n",
      "my:0.9; blast:0.1\n",
      "Have done bp F_max: 0.4509626488000532 Aupr: 0.3980121525342061 threadhold: 0.3\n",
      "my:0.8; blast:0.2\n",
      "Have done bp F_max: 0.45507797614991174 Aupr: 0.4046102646006762 threadhold: 0.35\n",
      "my:0.7; blast:0.3\n",
      "Have done bp F_max: 0.4602327106893186 Aupr: 0.4072672914921377 threadhold: 0.32\n",
      "my:0.6; blast:0.4\n",
      "Have done bp F_max: 0.46218646464748275 Aupr: 0.4068752886063506 threadhold: 0.31\n",
      "my:0.5; blast:0.5\n",
      "Have done bp F_max: 0.4622796637912327 Aupr: 0.404860868321875 threadhold: 0.26\n",
      "my:0.4; blast:0.6\n",
      "Have done bp F_max: 0.4584525521884285 Aupr: 0.39998367809181673 threadhold: 0.21\n",
      "my:0.30000000000000004; blast:0.7\n",
      "Have done bp F_max: 0.4537841271078317 Aupr: 0.3931685671565445 threadhold: 0.21\n",
      "my:0.19999999999999996; blast:0.8\n",
      "Have done bp F_max: 0.4384982631253121 Aupr: 0.3824159849705593 threadhold: 0.25\n",
      "my:0.09999999999999998; blast:0.9\n",
      "Have done bp F_max: 0.43445532212329907 Aupr: 0.3682002750907969 threadhold: 0.28\n"
     ]
    }
   ],
   "source": [
    "#先取min再与blast加权相加\n",
    "for ont in ['bp']:\n",
    "    model_num = '054'\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/valid_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/valid_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/valid_model_v{model_num}_{ont}_count_6.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/valid_model_v{model_num}_{ont}_count_7.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/valid_model_v{model_num}_{ont}_count_8.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/valid_model_v{model_num}_{ont}_count_9.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/valid_model_v{model_num}_{ont}_count_10.pkl')['preds']\n",
    "    res = [res1, res2, res3, res4, res5]\n",
    "    res =  np.mean(res, axis=0)\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, res, p_labels , ont)\n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_valid_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = float(s)\n",
    "        blast_r.append(r)\n",
    "        \n",
    "    for i in range(1,10):\n",
    "        print(f'my:{(1-i/10)}; blast:{i/10}')\n",
    "        r = np.array(res) * (1-i/10) + np.array(blast_r) * (i/10)\n",
    "        F_max, Aupr, threadhold = compute_performance_test(proteins, r, p_labels , ont)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp\n",
      "Have done bp F_max: 0.43779361766546543 Aupr: 0.3815448815460448 threadhold: 0.43\n",
      "Have done bp F_max: 0.4531010573787897 Aupr: 0.3994811449149222 threadhold: 0.33\n",
      "Have done bp F_max: 0.4572124509796894 Aupr: 0.39880470910699145 threadhold: 0.31\n",
      "Have done bp F_max: 0.45859385177015977 Aupr: 0.39517546769778505 threadhold: 0.28\n"
     ]
    }
   ],
   "source": [
    "#先取min再与blast加权相加\n",
    "for ont in ['bp']:\n",
    "    print(ont)\n",
    "    model_num = '054'\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/model_v{model_num}_{ont}_count_6.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/model_v{model_num}_{ont}_count_7.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/model_v{model_num}_{ont}_count_8.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/model_v{model_num}_{ont}_count_9.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/final_res/model_v{model_num}_{ont}_count_10.pkl')['preds']\n",
    "    res = [res1, res2, res3, res4, res5]\n",
    "    res =  np.mean(res, axis=0)\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, res, p_labels , ont)\n",
    "    with open(f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl', 'rb') as fr:\n",
    "        blast_res = pkl.load(fr)\n",
    "    blast_r = []\n",
    "    for p in proteins:\n",
    "        r = [0]*len(gos)\n",
    "        go_score = blast_res[p][ont]\n",
    "        for g,s in go_score.items():\n",
    "            r[go_idx[g]] = float(s)\n",
    "        blast_r.append(r)\n",
    "    \n",
    "    r = np.array(res) * 0.7 + np.array(blast_r) * 0.3\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, r, p_labels , ont)\n",
    "    r = np.array(res) * 0.6 + np.array(blast_r) * 0.4\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, r, p_labels , ont)\n",
    "    r = np.array(res) * 0.5 + np.array(blast_r) * 0.5\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, r, p_labels , ont)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test motif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ont in ['mf','cc']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res1 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_5.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v52.4_{ont}_count_4.pkl')['preds']\n",
    "    matrix_list = [ res2, res3, res4, res5]\n",
    "    final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "    # motif_iter = read_pkl(f'/public/home/hpc244706074/interpro_data/all_protein-motif_interpros.pkl')\n",
    "    # inter_go = read_pkl(f'/public/home/hpc244706074/interpro_data/all_interpro_go_motif.pkl')\n",
    "    # for i, p in enumerate(proteins):\n",
    "    #     iters = motif_iter[p]\n",
    "    #     anno_gos = []\n",
    "    #     for i in iters:\n",
    "    #         if i in inter_go:\n",
    "    #             for g in inter_go[i]:\n",
    "    #                 anno_gos.append(g)\n",
    "    #     for g in anno_gos:\n",
    "    #         if g in gos:\n",
    "    #             final_res[go_idx[g]] = 1\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    add_blast(ont, final_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ont in ['mf','cc','bp']:\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(gos)}\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    res = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_0.pkl')['preds']\n",
    "    res2 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_1.pkl')['preds']\n",
    "    res3 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_2.pkl')['preds']\n",
    "    res4 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_3.pkl')['preds']\n",
    "    res5 = read_pkl(f'/public/home/hpc244706074/myProject/results/model_v45.5_{ont}_fmax_4.pkl')['preds']\n",
    "    matrix_list = [res1, res2, res3, res4, res5]\n",
    "    final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res, p_labels , ont)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    final_res2 = get_final_res(ont, matrix_list , method='max')\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, final_res2, p_labels , ont)\n",
    "    print('mf max: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate blast/diamond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "mf F_max: 0.5654770978887889 Aupr: 0.3563765631388791 threadhold: 0.34\n",
      "cc\n",
      "cc F_max: 0.5589938234964529 Aupr: 0.3042110337568786 threadhold: 0.34\n",
      "bp\n",
      "bp F_max: 0.39549601298721476 Aupr: 0.19442102410088075 threadhold: 0.31\n"
     ]
    }
   ],
   "source": [
    "class_list = ['mf', 'cc', 'bp']\n",
    "# class_list = ['mf', 'cc']\n",
    "\n",
    "\n",
    "BASE_PATH = '/public/home/hpc244706074/myProject/dataset'\n",
    "for class_tag in class_list:\n",
    "    print(f'{class_tag}')\n",
    "    with open(f'/public/home/hpc244706074/diamond/data/diamond_test_predict_result.pkl','rb') as fr:\n",
    "        preds = pkl.load(fr)\n",
    "\n",
    "    proteins=list(pd.read_csv(f'{BASE_PATH}/{class_tag}/test_data_separate_{class_tag}_proteins.csv')['proteins'])\n",
    "    all_terms=list(pd.read_csv(f'{BASE_PATH}/select_min_count_1_{class_tag}_labels.csv')['functions'])\n",
    "\n",
    "    # term_file = f'{BASE_PATH}/{class_tag}_term.pkl'\n",
    "    # with open(term_file,\"rb\") as ft:\n",
    "    #     all_terms  = pkl.load(ft)\n",
    "        \n",
    "    #go 和 score一一对应\n",
    "    all_results = {}\n",
    "    for i in proteins:\n",
    "        all_results[i] = {}\n",
    "        # temp_res = {}\n",
    "        # for j in range(len(all_terms)):\n",
    "        #     if all_terms[j] in preds[i][class_tag]:\n",
    "        #         temp_res[all_terms[j]] = preds[i][class_tag][all_terms[j]]\n",
    "        #     else:\n",
    "        #         temp_res[all_terms[j]] = 0\n",
    "        # all_results[i] = temp_res\n",
    "        # if len(preds[i][class_tag]) == 0:\n",
    "        #        print(i)\n",
    "        all_results[i] = preds[i][class_tag]\n",
    "        \n",
    "    label_file=f'{BASE_PATH}/{class_tag}/test_data_separate_{class_tag}_labels_goname.pkl'\n",
    "    with open(label_file,\"rb\") as fp_label:\n",
    "        all_labels = pkl.load(fp_label)   #protein{[gos]}\n",
    "\n",
    "    go_file = f'{BASE_PATH}/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "    pred_gos = []\n",
    "    true_gos = []\n",
    "    for protein in proteins:\n",
    "        true_gos.append(all_labels[protein])\n",
    "        pred_gos.append(all_results[protein])\n",
    "\n",
    "    save_dict = {}\n",
    "    save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = true_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print(class_tag, 'F_max:', F_max,  'Aupr:', Aupr, 'threadhold:', threadhold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate tale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have done mf F_max: 0.13179634941398097 Aupr: 0.045121740140719485 threadhold: 0.13\n",
      "mf: aupr:0.045122,F_max:0.131796,threadhold:0.130000\n",
      "\n",
      "Have done cc F_max: 0.5377786032080801 Aupr: 0.3411096265005581 threadhold: 0.44\n",
      "mf: aupr:0.341110,F_max:0.537779,threadhold:0.440000\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/public/home/hpc244706074/TALE/output4/bp_tale_predict.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m proteins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/public/home/hpc244706074/myProject/dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mont\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test_data_separate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mont\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_proteins.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproteins\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# res = read_pkl(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_{ont}_{m}.pkl')\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mread_pkl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/public/home/hpc244706074/TALE/output4/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mont\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_tale_predict.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m p_labels \u001b[38;5;241m=\u001b[39m read_pkl(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/public/home/hpc244706074/myProject/dataset/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mont\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test_data_separate_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mont\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_labels_goname.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m pred_score \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[1], line 112\u001b[0m, in \u001b[0;36mread_pkl\u001b[0;34m(pklfile)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_pkl\u001b[39m(pklfile):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpklfile\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[1;32m    113\u001b[0m         data\u001b[38;5;241m=\u001b[39mpkl\u001b[38;5;241m.\u001b[39mload(fr)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/myproject/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/public/home/hpc244706074/TALE/output4/bp_tale_predict.pkl'"
     ]
    }
   ],
   "source": [
    "for ont in ['mf', 'cc', 'bp']:\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    # res = read_pkl(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_{ont}_{m}.pkl')\n",
    "    res = read_pkl(f'/public/home/hpc244706074/TALE/output4/{ont}_tale_predict.pkl')\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "\n",
    "    pred_score = []\n",
    "    for p in proteins:\n",
    "        pred_score.append(res[p])\n",
    "    # pred_score = res\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, pred_score, p_labels , ont)\n",
    "    print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                    (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have done mf F_max: 0.19830599243165767 Aupr: 0.07639512585663785 threadhold: 0.22\n",
      "mf: aupr:0.076395,F_max:0.198306,threadhold:0.220000\n",
      "\n",
      "Have done cc F_max: 0.5329422213401446 Aupr: 0.27608518656384845 threadhold: 0.49\n",
      "mf: aupr:0.276085,F_max:0.532942,threadhold:0.490000\n",
      "\n",
      "Have done bp F_max: 0.19349003175475776 Aupr: 0.10080055126068095 threadhold: 0.42\n",
      "mf: aupr:0.100801,F_max:0.193490,threadhold:0.420000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf', 'cc', 'bp']:\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    # res = read_pkl(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_{ont}_{m}.pkl')\n",
    "    res = read_pkl(f'/public/home/hpc244706074/TALE/output3/{ont}_tale_predict.pkl')\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "\n",
    "    pred_score = []\n",
    "    for p in proteins:\n",
    "        pred_score.append(res[p])\n",
    "    # pred_score = res\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, pred_score, p_labels , ont)\n",
    "    print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                    (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have done mf F_max: 0.32119764500037906 Aupr: 0.13057541645984846 threadhold: 0.38\n",
      "mf: aupr:0.130575,F_max:0.321198,threadhold:0.380000\n",
      "\n",
      "Have done cc F_max: 0.5837557715731664 Aupr: 0.37331031477586246 threadhold: 0.47\n",
      "mf: aupr:0.373310,F_max:0.583756,threadhold:0.470000\n",
      "\n",
      "Have done bp F_max: 0.2826597216100885 Aupr: 0.15875996985236973 threadhold: 0.48\n",
      "mf: aupr:0.158760,F_max:0.282660,threadhold:0.480000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf', 'cc', 'bp']:\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    # res = read_pkl(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_{ont}_{m}.pkl')\n",
    "    res = read_pkl(f'/public/home/hpc244706074/TALE/output/{ont}_tale+_predict.pkl')\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "\n",
    "    pred_score = []\n",
    "    for p in proteins:\n",
    "        pred_score.append(res[p])\n",
    "    # pred_score = res\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, pred_score, p_labels , ont)\n",
    "    print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                    (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have done mf F_max: 0.25435838864712 Aupr: 0.10188324913121276 threadhold: 0.4\n",
      "mf: aupr:0.101883,F_max:0.254358,threadhold:0.400000\n",
      "\n",
      "Have done cc F_max: 0.55409239067003 Aupr: 0.36432595375100835 threadhold: 0.58\n",
      "mf: aupr:0.364326,F_max:0.554092,threadhold:0.580000\n",
      "\n",
      "Have done bp F_max: 0.22010972705406462 Aupr: 0.11637000826682804 threadhold: 0.46\n",
      "mf: aupr:0.116370,F_max:0.220110,threadhold:0.460000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf', 'cc', 'bp']:\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    # res = read_pkl(f'/public/home/hpc244706074/myProject/dataset/other_methods_pre/test_{ont}_{m}.pkl')\n",
    "    res = read_pkl(f'/public/home/hpc244706074/TALE/output/{ont}_tale_predict.pkl')\n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "\n",
    "    pred_score = []\n",
    "    for p in proteins:\n",
    "        pred_score.append(res[p])\n",
    "    # pred_score = res\n",
    "    F_max, Aupr, threadhold = compute_performance_test(proteins, pred_score, p_labels , ont)\n",
    "    print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "                    (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n",
      "2604\n",
      "remained go num: 12\n",
      "remained protein num: 1006\n",
      "Have done cc F_max: 0.0 Aupr: nan threadhold: 1.0\n",
      "mf mean: aupr:nan,F_max:0.000000,threadhold:1.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ont in ['cc']:\n",
    "    print(ont)\n",
    "    zero_num = {'mf':56, 'bp':74, 'cc':21}\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    print(len(gos))\n",
    "    \n",
    "    p_labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels_goname.pkl')\n",
    "    # 筛选有多余GO的蛋白\n",
    "    remain_pro = set()\n",
    "    r_p_labels = {}\n",
    "    pro_idx = {}\n",
    "    min_c = 200\n",
    "    max_c = 2000\n",
    "    remain_gos = []\n",
    "    for g in gos:\n",
    "        c = 0\n",
    "        for p in proteins:\n",
    "            if g in p_labels[p]:\n",
    "                c = c+1\n",
    "        if c >= min_c and c <= max_c:\n",
    "            remain_gos.append(g)\n",
    "    print(f'remained go num: {len(remain_gos)}')\n",
    "    go_idx = {g:i for i,g in enumerate(remain_gos)}\n",
    "    # print(remain_gos)\n",
    "    r_g = []\n",
    "    for g in remain_gos:\n",
    "        r_g.append(go_idx[g])\n",
    "\n",
    "    for i, p in enumerate(proteins):\n",
    "        pro_idx[p] = i\n",
    "        for g in p_labels[p]:\n",
    "            if g in remain_gos:\n",
    "                remain_pro.add(p)\n",
    "            \n",
    "    print(f'remained protein num: {len(remain_pro)}')\n",
    "    for p in remain_pro:\n",
    "        r_p_labels[p] = []\n",
    "    \n",
    "    preds = read_pkl(f'/public/home/hpc244706074/compared_method_results/ATGO+_predict.pkl')\n",
    "    res = []\n",
    "    for p in remain_pro:\n",
    "        res_gs = preds[p][ont]\n",
    "        res_t = [0]*len(remain_gos)\n",
    "        for g in res_gs:\n",
    "            if g in remain_gos:\n",
    "                res_t[go_idx[g]]= float(res_gs[g])\n",
    "        res.append(res_t)\n",
    "        # print(res_t[r_g])\n",
    "        for g in p_labels[p]:\n",
    "            if g in remain_gos:\n",
    "                r_p_labels[p].append(g)\n",
    "        # print(r_p_labels[p])\n",
    "\n",
    "    F_max, Aupr, threadhold = compute_performance_zero(list(remain_pro), res, r_p_labels , ont, remain_gos)\n",
    "    print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))\n",
    "    # final_res2 = get_final_res(ont, matrix_list , method='max')\n",
    "    # F_max, Aupr, threadhold = compute_performance_test(proteins, final_res2, p_labels , ont)\n",
    "    # print('mf max: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = read_pkl(f'/public/home/hpc244706074/compared_method_results/ATGO+_predict.pkl')\n",
    "# print(len(res))\n",
    "# for i, r in enumerate(res):\n",
    "#     if i == 0:\n",
    "#         print(r)\n",
    "#         print(res[r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate cafa5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cafa1\n",
      "1\n",
      "mf\n",
      "7864\n",
      "Have done mf F_max: 0.7500935079791348 Aupr: 0.7045346294472258 threadhold: 0.51\n",
      "mf mean: aupr:0.704535,F_max:0.750094,threadhold:0.510000\n",
      "\n",
      "cc\n",
      "9292\n",
      "Have done cc F_max: 0.7531678204934454 Aupr: 0.8213536324951513 threadhold: 0.61\n",
      "mf mean: aupr:0.821354,F_max:0.753168,threadhold:0.610000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_n in ['cafa1']:\n",
    "    print(model_n)\n",
    "    for c in [1]:\n",
    "        print(c)\n",
    "        for ont in ['mf','cc']:\n",
    "            print(ont)\n",
    "            # proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "            # gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "            # go_idx = {g:i for i,g in enumerate(gos)}\n",
    "            \n",
    "            res1 = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/valid_model_v{model_n}_f0_{ont}_{c}.pkl')\n",
    "            res2 = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/valid_model_v{model_n}_f0_{ont}_{c+1}.pkl')\n",
    "            res3 = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/valid_model_v{model_n}_f0_{ont}_{c+2}.pkl')\n",
    "            res4 = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/valid_model_v{model_n}_f0_{ont}_{c+3}.pkl')\n",
    "            res5 = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/valid_model_v{model_n}_f0_{ont}_{c+4}.pkl')\n",
    "            proteins = res1['proteins']\n",
    "            all_labels = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/Train/{ont}_labels_onehot.pkl')\n",
    "            val_labels = [all_labels[p] for p in proteins]\n",
    "            matrix_list = [res1['preds'], res2['preds'], res3['preds'], res4['preds'], res5['preds']]\n",
    "            final_res = get_final_res(ont, matrix_list , method='mean')\n",
    "            print(len(final_res))\n",
    "            F_max, Aupr, threadhold = compute_performance_cafa(proteins, final_res, val_labels , ont)\n",
    "            print('mf mean: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % ((Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "def read_pkl(input_file):\n",
    "    with open(input_file,'rb') as fr:\n",
    "        temp_result = pkl.load(fr)\n",
    "    return temp_result\n",
    "\n",
    "def save_pkl(pklfile, data):\n",
    "    with open(pklfile,'wb') as fw:\n",
    "        pkl.dump(data, fw)\n",
    "\n",
    "def get_final_res(ont, matrix_list , method='mean'):\n",
    "    if method == 'mean':\n",
    "        res = np.mean(matrix_list, axis=0)\n",
    "    elif method == 'max':\n",
    "        res = np.max(matrix_list, axis=0)\n",
    "    return res\n",
    "cc_res = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/model_vcafa1_f0_cc_1_5combine.pkl')['preds']\n",
    "bp_res = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/model_vcafa1_f0_bp_1_5combine.pkl')['preds']\n",
    "mf_res = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/model_vcafa1_f0_mf_1_5combine.pkl')['preds']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing res:   0%|          | 31/141864 [00:12<16:17:37,  2.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line -1\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "go_file = '/public/home/hpc244706074/myProject/CAFA/Train/go-basic.obo'\n",
    "go = Ontology(go_file, with_rels=True)\n",
    "proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/CAFA/Test/all_test_proteins.csv')['proteins'])\n",
    "for ont in ['mf','cc','bp']:\n",
    "    res = read_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/model_vcafa1_f0_{ont}_5combine.pkl')['preds']\n",
    "    select_gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/CAFA/Train/{ont}_labels.csv')['functions'])\n",
    "    s_goidx = {g:i for i,g in enumerate(select_gos)}\n",
    "    go_set = go.get_namespace_terms(NAMESPACES[ont])\n",
    "    labels = list(go_set)\n",
    "    goid_idx = {}\n",
    "    idx_goid = {}\n",
    "    for idx, goid in enumerate(labels):\n",
    "        goid_idx[goid] = idx\n",
    "        idx_goid[idx] = goid\n",
    "\n",
    "    pred_scores = {}\n",
    "    # Annotations\n",
    "    for i, r in enumerate(tqdm(res, desc=\"Processing res\")):\n",
    "        pred_scores[proteins[i]] = {}\n",
    "        # pred\n",
    "        vals = [-1]*len(labels)\n",
    "        for j,score in enumerate(r):\n",
    "    #             print(f'item:{items},score:{score}, vals:{vals[goid_idx[items]]}')\n",
    "            score = round(float(score), 3)\n",
    "            if score < 0.1:\n",
    "                continue\n",
    "            if select_gos[j] in go_set:\n",
    "                vals[goid_idx[select_gos[j]]] = max(score, vals[goid_idx[select_gos[j]]])\n",
    "            go_parent = go.get_anchestors(select_gos[j])\n",
    "            for go_id in go_parent:\n",
    "                if go_id in go_set:\n",
    "                    vals[goid_idx[go_id]] = max(vals[goid_idx[go_id]], score)\n",
    "        val_temp = [0]*len(select_gos)\n",
    "        for j,g in enumerate(select_gos):\n",
    "            if g in select_gos:\n",
    "                pred_scores[proteins[i]][g] = vals[goid_idx[g]]\n",
    "    \n",
    "    save_pkl(f'/public/home/hpc244706074/myProject/CAFA/results/model_vcafa1_f0_{ont}_5combine_propose.pkl',pred_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照频率划分测试集并传播结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "def read_pkl(input_file):\n",
    "    with open(input_file,'rb') as fr:\n",
    "        temp_result = pkl.load(fr)\n",
    "    return temp_result\n",
    "\n",
    "def save_pkl(pklfile, data):\n",
    "    with open(pklfile,'wb') as fw:\n",
    "        pkl.dump(data, fw)\n",
    "\n",
    "def get_final_res(matrix_list , method='mean'):\n",
    "    if method == 'mean':\n",
    "        res = np.mean(matrix_list, axis=0)\n",
    "    elif method == 'max':\n",
    "        res = np.max(matrix_list, axis=0)\n",
    "    elif method == 'sum':\n",
    "        res = np.sum(matrix_list, axis=0)\n",
    "    return res\n",
    "\n",
    "def split_go_freq(min_n, max_n, ont, all_pros):\n",
    "    labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/train_data_separate_{ont}_labels.pkl')\n",
    "    onehot = [o for p,o in labels.items()]\n",
    "    count = get_final_res(onehot,method='sum')\n",
    "    print(f'go_num: {len(count)}')\n",
    "    select_gos = np.array(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    num_list = []\n",
    "    for i, c in enumerate(count):\n",
    "        if c>min_n and c<=max_n:\n",
    "            num_list.append(i)\n",
    "    print(f'max idx: {max(num_list)}')\n",
    "    pro_list = []\n",
    "    for i, l in enumerate(np.array(true_labels)[:,num_list]):\n",
    "        if np.sum(l)>0:\n",
    "            pro_list.append(i)\n",
    "    return num_list, select_gos[num_list], pro_list, all_pros[pro_list]\n",
    "\n",
    "\n",
    "# def get_selected_res(preds, true_labels,):\n",
    "#     preds = preds[pro_list,:]\n",
    "#     true_labels = true_labels[pro_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go_num: 18832\n",
      "max idx: 15172\n",
      "selected gonum: 3403\n",
      "selected pronum: 1313\n",
      "atgo\n",
      "(1313, 18832)\n",
      "atgo+\n",
      "(1313, 18832)\n",
      "deepgozero\n",
      "blastKNN\n"
     ]
    }
   ],
   "source": [
    "# 按照频率划分后生成文件 onehot  goterm  protein preds\n",
    "for ont in ['bp']:\n",
    "    model_file = {'dugpro':f'/public/home/hpc244706074/myProject/final_res/dugpro_res_{ont}.pkl',\n",
    "                  'dugpro+':f'/public/home/hpc244706074/myProject/final_res/dugpro+_res_{ont}.pkl',\n",
    "                  'deepgose':f'/public/home/hpc244706074/compared_method_results/deepgose_{ont}_res.pkl',\n",
    "                 'atgo':f'/public/home/hpc244706074/compared_method_results/ATGO_predict.pkl',\n",
    "                 'atgo+':f'/public/home/hpc244706074/compared_method_results/ATGO+_predict.pkl',\n",
    "                 'deepgozero':f'/public/home/hpc244706074/compared_method_results/deepgozero_{ont}_res.pkl',\n",
    "                 'tale':f'/public/home/hpc244706074/TALE/output3/{ont}_tale_predict.pkl',\n",
    "                 'tale+':f'/public/home/hpc244706074/TALE/output3/{ont}_tale+_predict.pkl',\n",
    "                 'blastKNN':f'/public/home/hpc244706074/compared_method_results/deepgose_{ont}_res.pkl'}\n",
    "    proteins=list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels.pkl')\n",
    "    all_terms=list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(all_terms)}\n",
    "    true_labels = [o for p,o in labels.items()]\n",
    "    true_labels = np.array(true_labels)\n",
    "    min_n = 50\n",
    "    max_n = 1000000000\n",
    "    select_goidx, goname, pro_list, seleted_proteins = split_go_freq(min_n, max_n, ont, np.array(proteins))\n",
    "    print(f'selected gonum: {len(goname)}')\n",
    "    print(f'selected pronum: {len(seleted_proteins)}')\n",
    "    trues = true_labels[pro_list,:]\n",
    "    trues = trues[:,select_goidx]\n",
    "    # save_pkl(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_onehot.pkl',trues)\n",
    "    pros = pd.DataFrame({'proteins':list(seleted_proteins)})\n",
    "    # pros.to_csv(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{ont}_proteins.csv')\n",
    "    df = pd.DataFrame({'functions':list(goname)})\n",
    "    # df.to_csv(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{ont}_terms.csv')\n",
    "    # for model in ['dugpro+','dugpro','deepgose','atgo', 'atgo+', 'deepgozero', 'blastKNN']:\n",
    "    for model in ['atgo', 'atgo+', 'deepgozero', 'blastKNN']:\n",
    "        print(model)\n",
    "        if model == 'dugpro' or model == 'dugpro+':\n",
    "            preds = read_pkl(model_file[model])['preds']\n",
    "        elif model == 'atgo' or model == 'atgo+':\n",
    "            res = read_pkl(model_file[model])\n",
    "            preds = []\n",
    "            for i,p in enumerate(proteins):\n",
    "                temp = [0]*len(all_terms)\n",
    "                go_pre = res[p][ont]\n",
    "                for g in go_pre:\n",
    "                    if g in go_idx:\n",
    "                        temp[go_idx[g]] = float(go_pre[g])\n",
    "                preds.append(temp)\n",
    "            preds = np.array(preds)\n",
    "            print(preds.shape)\n",
    "        elif model == 'tale':\n",
    "            preds = read_pkl(model_file[model])\n",
    "        elif model == 'deepgozero' or model == 'deepgose' or model == 'blastKNN':\n",
    "            res = read_pkl(model_file[model])\n",
    "            preds = np.array([res[p] for p in proteins])\n",
    "            \n",
    "        preds = preds[pro_list,:]\n",
    "        preds = preds[:,select_goidx]\n",
    "        trues = true_labels[pro_list,:]\n",
    "        trues = trues[:,select_goidx]\n",
    "\n",
    "        save_pkl(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{model}_res.csv',preds)\n",
    "        # fmax, aupr, t = compute_performance_gofreq(seleted_proteins, preds, trues, ont, goname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "blastKNN\n",
      "(703, 6091)\n",
      "gonum: 6091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing res: 100%|██████████| 703/703 [01:28<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cc\n",
      "blastKNN\n",
      "(1006, 2604)\n",
      "gonum: 2604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing res: 100%|██████████| 1006/1006 [00:48<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp\n",
      "blastKNN\n",
      "(1313, 18832)\n",
      "gonum: 18832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing res: 100%|██████████| 1313/1313 [07:19<00:00,  2.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# 传播结果\n",
    "from tqdm import tqdm\n",
    "go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "go = Ontology(go_file, with_rels=True)\n",
    "for ont in ['mf','cc','bp']:\n",
    "    model_file = {'dugpro':f'/public/home/hpc244706074/myProject/final_res/dugpro_res_{ont}.pkl',\n",
    "                  'dugpro+':f'/public/home/hpc244706074/myProject/final_res/dugpro+_res_{ont}.pkl',\n",
    "                  'deepgose':f'/public/home/hpc244706074/compared_method_results/deepgose_{ont}_res.pkl',\n",
    "                 'atgo':f'/public/home/hpc244706074/compared_method_results/ATGO_predict.pkl',\n",
    "                 'atgo+':f'/public/home/hpc244706074/compared_method_results/ATGO+_predict.pkl',\n",
    "                 'deepgozero':f'/public/home/hpc244706074/compared_method_results/deepgozero_{ont}_res.pkl',\n",
    "                 'tale':f'/public/home/hpc244706074/TALE/output3/{ont}_tale_predict.pkl',\n",
    "                 'tale+':f'/public/home/hpc244706074/TALE/output3/{ont}_tale+_predict.pkl',\n",
    "                 'blastKNN':f'/public/home/hpc244706074/compared_method_results/blast_{ont}_predict_5000.pkl'}\n",
    "    print(ont)\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    select_gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i,g in enumerate(select_gos)}\n",
    "    # for model in ['dugpro+','dugpro','deepgose','atgo', 'atgo+', 'deepgozero', 'blastKNN']:\n",
    "    for model in ['blastKNN']:\n",
    "        print(model)\n",
    "        if model == 'dugpro' or model == 'dugpro+':\n",
    "            preds = read_pkl(model_file[model])['preds']\n",
    "        elif model == 'atgo' or model == 'atgo+' or model == 'blastKNN':\n",
    "            res = read_pkl(model_file[model])\n",
    "            preds = []\n",
    "            for i,p in enumerate(proteins):\n",
    "                temp = [0]*len(select_gos)\n",
    "                go_pre = res[p][ont]\n",
    "                for g in go_pre:\n",
    "                    if g in go_idx:\n",
    "                        temp[go_idx[g]] = float(go_pre[g])\n",
    "                preds.append(temp)\n",
    "            preds = np.array(preds)\n",
    "            print(preds.shape)\n",
    "        elif model == 'tale':\n",
    "            preds = read_pkl(model_file[model])\n",
    "        elif model == 'deepgozero' or model == 'deepgose':\n",
    "            res = read_pkl(model_file[model])\n",
    "            preds = np.array([res[p] for p in proteins])\n",
    "\n",
    "        print(f'gonum: {len(select_gos)}')\n",
    "        s_goidx = {g:i for i,g in enumerate(select_gos)}\n",
    "        go_set = go.get_namespace_terms(NAMESPACES[ont])\n",
    "        labels = list(go_set)\n",
    "        goid_idx = {}\n",
    "        idx_goid = {}\n",
    "        for idx, goid in enumerate(labels):\n",
    "            goid_idx[goid] = idx\n",
    "            idx_goid[idx] = goid\n",
    "\n",
    "        pred_scores = []\n",
    "        # Annotations\n",
    "        for i, r in enumerate(tqdm(preds, desc=\"Processing res\")):\n",
    "            # pred\n",
    "            vals = [-1]*len(labels)\n",
    "            for j,score in enumerate(r):\n",
    "        #             print(f'item:{items},score:{score}, vals:{vals[goid_idx[items]]}')\n",
    "                score = float(score)\n",
    "                # if score < 0.1:\n",
    "                #     continue\n",
    "                if select_gos[j] in go_set:\n",
    "                    vals[goid_idx[select_gos[j]]] = max(score, vals[goid_idx[select_gos[j]]])\n",
    "                go_parent = go.get_anchestors(select_gos[j])\n",
    "                for go_id in go_parent:\n",
    "                    if go_id in go_set:\n",
    "                        vals[goid_idx[go_id]] = max(vals[goid_idx[go_id]], score)\n",
    "            val_temp = [0]*len(select_gos)\n",
    "            for j,g in enumerate(select_gos):\n",
    "                # if g in select_gos:\n",
    "                val_temp[j] = vals[goid_idx[g]]\n",
    "            pred_scores.append(val_temp)\n",
    "        save_pkl(f'/public/home/hpc244706074/compared_method_results/{model}_{ont}_res_proposed.pkl',pred_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 按照GO深度划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "def read_pkl(input_file):\n",
    "    with open(input_file,'rb') as fr:\n",
    "        temp_result = pkl.load(fr)\n",
    "    return temp_result\n",
    "\n",
    "def save_pkl(pklfile, data):\n",
    "    with open(pklfile,'wb') as fw:\n",
    "        pkl.dump(data, fw)\n",
    "\n",
    "def get_final_res(matrix_list , method='mean'):\n",
    "    if method == 'mean':\n",
    "        res = np.mean(matrix_list, axis=0)\n",
    "    elif method == 'max':\n",
    "        res = np.max(matrix_list, axis=0)\n",
    "    elif method == 'sum':\n",
    "        res = np.sum(matrix_list, axis=0)\n",
    "    return res\n",
    "\n",
    "def split_go_depth(min_n, max_n, ont):\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    all_terms = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    go_idx = {g:i for i, g in enumerate(all_terms)}\n",
    "    go_depth = [0]*len(all_terms)\n",
    "    for g in all_terms:\n",
    "        go_depth[go_idx[g]] = go.get_depth(g,ont)\n",
    "    go_depth = np.array(go_depth)\n",
    "    go_list = np.where((go_depth > min_n) & (go_depth <= max_n))[0]\n",
    "    print(go_list)\n",
    "    labels = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels.pkl')\n",
    "    onehot = [o for p,o in labels.items()]\n",
    "    onehot = np.array(onehot)\n",
    "    onehot = onehot[:,go_list]\n",
    "    pro_list = []\n",
    "    for i in range(len(onehot)):\n",
    "        if sum(onehot[i]) > 0:\n",
    "            pro_list.append(i)\n",
    "    \n",
    "    return go_list, pro_list\n",
    "\n",
    "\n",
    "# def get_selected_res(preds, true_labels,):\n",
    "#     preds = preds[pro_list,:]\n",
    "#     true_labels = true_labels[pro_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 传播结果\n",
    "from tqdm import tqdm\n",
    "go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "go = Ontology(go_file, with_rels=True)\n",
    "\n",
    "for ont in ['mf','cc','bp']:\n",
    "    print(ont)\n",
    "    for k in range(len(min_list)):\n",
    "        min_n = min_list[k]\n",
    "        max_n = max_list[k]\n",
    "        proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{ont}_proteins.csv')['proteins'])\n",
    "        for model in ['dugpro+','dugpro','deepgose','atgo', 'atgo+', 'deepgozero', 'blastKNN']:\n",
    "            print(model)\n",
    "            res_file = f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{model}_res.csv'\n",
    "            res = read_pkl(res_file)\n",
    "            select_gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{ont}_terms.csv')['functions'])\n",
    "            s_goidx = {g:i for i,g in enumerate(select_gos)}\n",
    "            go_set = go.get_namespace_terms(NAMESPACES[ont])\n",
    "            labels = list(go_set)\n",
    "            goid_idx = {}\n",
    "            idx_goid = {}\n",
    "            for idx, goid in enumerate(labels):\n",
    "                goid_idx[goid] = idx\n",
    "                idx_goid[idx] = goid\n",
    "\n",
    "            pred_scores = []\n",
    "            # Annotations\n",
    "            for i, r in enumerate(tqdm(res, desc=\"Processing res\")):\n",
    "                # pred\n",
    "                vals = [-1]*len(labels)\n",
    "                for j,score in enumerate(r):\n",
    "            #             print(f'item:{items},score:{score}, vals:{vals[goid_idx[items]]}')\n",
    "                    score = round(float(score), 3)\n",
    "                    # if score < 0.1:\n",
    "                    #     continue\n",
    "                    if select_gos[j] in go_set:\n",
    "                        vals[goid_idx[select_gos[j]]] = max(score, vals[goid_idx[select_gos[j]]])\n",
    "                    go_parent = go.get_anchestors(select_gos[j])\n",
    "                    for go_id in go_parent:\n",
    "                        if go_id in go_set:\n",
    "                            vals[goid_idx[go_id]] = max(vals[goid_idx[go_id]], score)\n",
    "                val_temp = [0]*len(select_gos)\n",
    "                for j,g in enumerate(select_gos):\n",
    "                    # if g in select_gos:\n",
    "                    val_temp[j] = vals[goid_idx[g]]\n",
    "                pred_scores.append(val_temp)\n",
    "            save_pkl(f'/public/home/hpc244706074/myProject/go_freq/{ont}/{min_n}-{max_n}_{model}_res.pkl',pred_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pkl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_pkl\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/public/home/hpc244706074/myProject/dataset/mf/train_data_separate_mf_sequences.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i ,p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m :\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pkl' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy.sparse as ssp\n",
    "# from sklearn.metrics import average_precision_score as aupr\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import OrderedDict,deque,Counter\n",
    "import math\n",
    "import re\n",
    "import pickle as pkl\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='uniprot_API')\n",
    "    parser.add_argument('--predict')\n",
    "    parser.add_argument('--output_path')\n",
    "    parser.add_argument('--true')\n",
    "    parser.add_argument('--background')\n",
    "    parser.add_argument('--go')\n",
    "    parser.add_argument('--metrics')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "__all__ = ['fmax', 'aupr', 'ROOT_GO_TERMS', 'compute_performance', 'compute_performance_deepgoplus', 'read_pkl', 'save_pkl']\n",
    "ROOT_GO_TERMS = {'GO:0003674', 'GO:0008150', 'GO:0005575'}\n",
    "\n",
    "def fmax(go, targets, scores, idx_goid):\n",
    "    targets = ssp.csr_matrix(targets)\n",
    "    \n",
    "    fmax_ = 0.0, 0.0, 0.0\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    icprecisions = []\n",
    "    icrecalls = []\n",
    "    dpprecisions = []\n",
    "    dprecalls = []\n",
    "    goic_list=[]\n",
    "    godp_list=[]\n",
    "    for i in range(len(idx_goid)):\n",
    "        goic_list.append(go.get_ic(idx_goid[i]))\n",
    "    for i in range(len(idx_goid)):\n",
    "        godp_list.append(go.get_icdepth(idx_goid[i]))\n",
    "    goic_vector=np.array(goic_list).reshape(-1,1)\n",
    "    godp_vector=np.array(godp_list).reshape(-1,1)\n",
    "    \n",
    "    for cut in (c / 100 for c in range(101)):\n",
    "        cut_sc = ssp.csr_matrix((scores >= cut).astype(np.int32))\n",
    "        correct = cut_sc.multiply(targets).sum(axis=1)\n",
    "        correct_sc = cut_sc.multiply(targets)\n",
    "        fp_sc = cut_sc-correct_sc\n",
    "        fn_sc = targets-correct_sc\n",
    "        \n",
    "        correct_ic=ssp.csr_matrix(correct_sc.dot(goic_vector))\n",
    "        cut_ic=ssp.csr_matrix(cut_sc.dot(goic_vector))\n",
    "        targets_ic=ssp.csr_matrix(targets.dot(goic_vector))\n",
    "        \n",
    "        correct_dp=ssp.csr_matrix(correct_sc.dot(godp_vector))\n",
    "        cut_dp=ssp.csr_matrix(cut_sc.dot(godp_vector))\n",
    "        targets_dp=ssp.csr_matrix(targets.dot(godp_vector))\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            p, r = correct / cut_sc.sum(axis=1), correct / targets.sum(axis=1)\n",
    "            p, r = np.average(p[np.invert(np.isnan(p))]), np.average(r)\n",
    "            \n",
    "            mi=fp_sc.dot(goic_vector).sum(axis=0)\n",
    "            ru=fn_sc.dot(goic_vector).sum(axis=0)\n",
    "            mi/=len(targets.sum(axis=1))\n",
    "            ru/=len(targets.sum(axis=1))\n",
    "            \n",
    "            icp, icr = correct_ic/cut_ic, correct_ic/targets_ic\n",
    "            icp, icr = np.average(icp[np.invert(np.isnan(icp))]), np.average(icr)\n",
    "            \n",
    "            dpp, dpr = correct_dp/cut_dp, correct_dp/targets_dp\n",
    "            dpp, dpr = np.average(dpp[np.invert(np.isnan(dpp))]), np.average(dpr)\n",
    "            \n",
    "        if np.isnan(p):\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(r)\n",
    "        else:\n",
    "            precisions.append(p)\n",
    "            recalls.append(r)\n",
    "            \n",
    "        if np.isnan(icp):\n",
    "            icprecisions.append(0.0)\n",
    "            icrecalls.append(icr)\n",
    "        else:\n",
    "            icprecisions.append(icp)\n",
    "            icrecalls.append(icr)\n",
    "            \n",
    "        if np.isnan(dpp):\n",
    "            dpprecisions.append(0.0)\n",
    "            dprecalls.append(dpr)\n",
    "        else:\n",
    "            dpprecisions.append(dpp)\n",
    "            dprecalls.append(dpr)\n",
    "        \n",
    "        try:\n",
    "            fmax_ = max(fmax_, (2 * p * r / (p + r) if p + r > 0.0 else 0.0, math.sqrt(ru*ru + mi*mi) , cut))\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "    return fmax_[0], fmax_[1], fmax_[2], precisions, recalls, icprecisions, icrecalls, dpprecisions, dprecalls\n",
    "\n",
    "def read_pkl(pklfile):\n",
    "    with open(pklfile,'rb') as fr:\n",
    "        data=pkl.load(fr)\n",
    "    return data\n",
    "\n",
    "def save_pkl(pklfile, data):\n",
    "    with open(pklfile,'wb') as fw:\n",
    "        pkl.dump(data, fw)\n",
    "\n",
    "BIOLOGICAL_PROCESS = 'GO:0008150'\n",
    "MOLECULAR_FUNCTION = 'GO:0003674'\n",
    "CELLULAR_COMPONENT = 'GO:0005575'\n",
    "FUNC_DICT = {\n",
    "    'cc': CELLULAR_COMPONENT,\n",
    "    'mf': MOLECULAR_FUNCTION,\n",
    "    'bp': BIOLOGICAL_PROCESS}\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}\n",
    "\n",
    "NAMESPACES_REVERT={\n",
    "    'cellular_component': 'cc',\n",
    "    'molecular_function': 'mf',\n",
    "    'biological_process': 'bp'\n",
    "}\n",
    "\n",
    "EXP_CODES = set(['EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC',])\n",
    "CAFA_TARGETS = set([\n",
    "    '10090', '223283', '273057', '559292', '85962',\n",
    "    '10116',  '224308', '284812', '7227', '9606',\n",
    "    '160488', '237561', '321314', '7955', '99287',\n",
    "    '170187', '243232', '3702', '83333', '208963',\n",
    "    '243273', '44689', '8355'])\n",
    "\n",
    "def is_cafa_target(org):\n",
    "    return org in CAFA_TARGETS\n",
    "\n",
    "def is_exp_code(code):\n",
    "    return code in EXP_CODES\n",
    "\n",
    "class Ontology(object):\n",
    "    def __init__(self, filename, with_rels=False):\n",
    "        self.ont = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "        self.icdepth=None\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        self.icdepth={}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "            self.icdepth[go_id]=math.log(self.get_depth(go_id,NAMESPACES_REVERT[self.get_namespace(go_id)]),2)*self.ic[go_id]\n",
    "    \n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "    \n",
    "    def get_icdepth(self, go_id):\n",
    "        if self.icdepth is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.icdepth:\n",
    "            return 0.0\n",
    "        return self.icdepth[go_id]\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['is_a'].append(it[1])\n",
    "                            \n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "        if obj is not None:\n",
    "            ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont\n",
    "\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while(len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "    \n",
    "    def get_depth(self,term_id,ont):\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        layer=1\n",
    "        while(len(q) > 0):\n",
    "            all_p=set()\n",
    "            while(len(q)>0):\n",
    "                t_id = q.popleft()\n",
    "                p_id=self.get_parents(t_id)\n",
    "                all_p.update(p_id)\n",
    "            if all_p:\n",
    "                layer+=1\n",
    "                for item in all_p:\n",
    "                    if item == FUNC_DICT[ont]:\n",
    "                        return layer\n",
    "                    q.append(item)\n",
    "        return layer\n",
    "\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        return self.ont[term_id]['namespace']\n",
    "    \n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "        \n",
    "def new_compute_performance(test_df, go, ont):\n",
    "\n",
    "    go_set = go.get_namespace_terms(NAMESPACES[ont])\n",
    "    go_set.remove(FUNC_DICT[ont])\n",
    "    # print(len(go_set))\n",
    "    \n",
    "    labels = list(go_set)\n",
    "    goid_idx = {}\n",
    "    idx_goid = {}\n",
    "    for idx, goid in enumerate(labels):\n",
    "        goid_idx[goid] = idx\n",
    "        idx_goid[idx] = goid\n",
    "\n",
    "    pred_scores = []\n",
    "    true_scores = []\n",
    "    # Annotations\n",
    "    for i, row in enumerate(test_df.itertuples()):\n",
    "        # true\n",
    "        vals = [0]*len(labels)\n",
    "        annots = set()\n",
    "        for go_id in row.gos:\n",
    "            if go.has_term(go_id):\n",
    "                annots |= go.get_anchestors(go_id)\n",
    "        for go_id in annots:\n",
    "            if go_id in go_set:\n",
    "                vals[goid_idx[go_id]] = 1\n",
    "        if sum(vals) == 0:\n",
    "            print(i)\n",
    "        true_scores.append(vals)\n",
    "\n",
    "        # pred\n",
    "        vals = [-1]*len(labels)\n",
    "        for items,score in row.predictions.items():\n",
    "            if items in go_set:\n",
    "                vals[goid_idx[items]] = max(score, vals[goid_idx[items]])\n",
    "            go_parent = go.get_anchestors(items)\n",
    "            for go_id in go_parent:\n",
    "                if go_id in go_set:\n",
    "                    vals[goid_idx[go_id]] = max(vals[goid_idx[go_id]], score)\n",
    "        pred_scores.append(vals)\n",
    "    pred_scores = np.array(pred_scores)\n",
    "    true_scores = np.array(true_scores)\n",
    "    # print(pred_scores.shape, true_scores.shape, sum(pred_scores<0), sum(pred_scores>0))\n",
    "    \n",
    "    result_fmax, result_smin, result_t, precisions, recalls, icprecisions, icrecalls, dpprecisions, dprecalls = fmax(go, true_scores, pred_scores, idx_goid)\n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    sorted_index = np.argsort(recalls)\n",
    "    recalls = recalls[sorted_index]\n",
    "    precisions = precisions[sorted_index]\n",
    "    result_aupr = np.trapz(precisions, recalls)\n",
    "    \n",
    "    icprecisions = np.array(icprecisions)\n",
    "    icrecalls = np.array(icrecalls)\n",
    "    sorted_index = np.argsort(icrecalls)\n",
    "    icrecalls = icrecalls[sorted_index]\n",
    "    icprecisions = icprecisions[sorted_index]\n",
    "    result_icaupr = np.trapz(icprecisions, icrecalls)\n",
    "    \n",
    "    dpprecisions = np.array(dpprecisions)\n",
    "    dprecalls = np.array(dprecalls)\n",
    "    sorted_index = np.argsort(dprecalls)\n",
    "    dprecalls = dprecalls[sorted_index]\n",
    "    dpprecisions = dpprecisions[sorted_index]\n",
    "    result_dpaupr = np.trapz(dpprecisions, dprecalls)\n",
    "\n",
    "    return result_fmax, result_smin , result_aupr, result_icaupr, result_dpaupr, result_t\n",
    "\n",
    "def compute_performance_test(go, predscore, true_gos, class_tag):\n",
    "\n",
    "    file_path = f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{class_tag}_labels.csv'\n",
    "    all_go = list(pd.read_csv(file_path)['functions'])\n",
    "\n",
    "    pred_gos = []\n",
    "    for pred in predscore:\n",
    "        pred_go = {}\n",
    "        for i ,score in enumerate(pred):\n",
    "            pred_go[all_go[i]] = float(score)\n",
    "        pred_gos.append(pred_go)\n",
    "    \n",
    "    all_gos = []\n",
    "    # for p in true_gos:\n",
    "    #     gos = [g for g in true_gos[p]]\n",
    "    #     all_gos.append(gos)\n",
    "    for p in true_gos:\n",
    "        gos = []\n",
    "        for i, l in enumerate(true_gos[p]):\n",
    "            if l==1:\n",
    "                gos.append(all_go[i])\n",
    "        all_gos.append(gos)\n",
    "    save_dict = {}\n",
    "    # save_dict['protein_id'] = proteins\n",
    "    save_dict['gos'] = all_gos\n",
    "    save_dict['predictions'] = pred_gos\n",
    "\n",
    "    df = pd.DataFrame(save_dict)\n",
    "    # F_max, Aupr, threadhold = new_compute_performance(df, go, class_tag)\n",
    "    result_fmax, result_smin , result_aupr, result_icaupr, result_dpaupr, result_t = new_compute_performance(df, go, class_tag)\n",
    "\n",
    "    print('Have done', class_tag, \n",
    "          'F_max:', result_fmax, \n",
    "          'Aupr:', result_aupr, \n",
    "          'threadhold:', result_t,\n",
    "          'Smin:', result_smin,\n",
    "          'ICAupr:', result_icaupr,\n",
    "          'DPAupr:', result_dpaupr,\n",
    "         )\n",
    "\n",
    "    return result_fmax, result_smin , result_aupr, result_icaupr, result_dpaupr, result_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bp\n",
      "dugpro+\n",
      "Have done bp F_max: 0.45778775904954444 Aupr: 0.40007023852708273 threadhold: 0.31 Smin: 28.73401907147096 ICAupr: 0.33879271888783363 DPAupr: 0.3125085340855912\n",
      "dugpro\n",
      "Have done bp F_max: 0.4383102407738841 Aupr: 0.3829234341628669 threadhold: 0.43 Smin: 27.782167397561622 ICAupr: 0.3215030925331013 DPAupr: 0.2941075521434575\n",
      "deepgose\n",
      "Have done bp F_max: 0.4217552381147045 Aupr: 0.3706155645109942 threadhold: 0.29 Smin: 29.334816315914882 ICAupr: 0.2988040991923742 DPAupr: 0.26881822769317537\n",
      "atgo\n",
      "Have done bp F_max: 0.37387111971503195 Aupr: 0.3234570576274562 threadhold: 0.22 Smin: 28.487297323771113 ICAupr: 0.24955944759760063 DPAupr: 0.2140007216376093\n",
      "atgo+\n",
      "Have done bp F_max: 0.4437589641760166 Aupr: 0.38024723632544066 threadhold: 0.24 Smin: 27.072555721517535 ICAupr: 0.31316057709938216 DPAupr: 0.2826594084002493\n",
      "deepgozero\n",
      "Have done bp F_max: 0.39670748895310987 Aupr: 0.3285093710824755 threadhold: 0.26 Smin: 30.772579495146143 ICAupr: 0.25839347337387647 DPAupr: 0.22922201127719571\n",
      "blastKNN\n",
      "Have done bp F_max: 0.41138609973109047 Aupr: 0.29011536191565224 threadhold: 0.29 Smin: 29.578341399356262 ICAupr: 0.24967641345752134 DPAupr: 0.23416214902372162\n"
     ]
    }
   ],
   "source": [
    "for ont in ['bp']: \n",
    "    print(ont)\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    data = read_pkl('/public/home/hpc244706074/myProject/dataset/train_data_separate.pkl')\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/train_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    all_annotations=[]\n",
    "    for p in data:\n",
    "        item_set=set()\n",
    "        for item in data[p]['annotations']:\n",
    "            item=item.split('|')[0]\n",
    "            if go.has_term(item):\n",
    "                item_set |= go.get_anchestors(item)\n",
    "        all_annotations.append(list(item_set))\n",
    "    go.calculate_ic(all_annotations)\n",
    "    p_gos = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels.pkl')\n",
    "    s_plabels = {}\n",
    "    \n",
    "    for i,p in enumerate(p_gos):\n",
    "        if p != 'ATPF1_MOUSE':\n",
    "            s_plabels[p] = p_gos[p]\n",
    "        else:\n",
    "            del_id = i\n",
    "    model_names = ['dugpro+', 'dugpro', 'deepgose', 'atgo', 'atgo+', 'deepgozero', 'blastKNN']\n",
    "    for n in model_names:\n",
    "        print(n)\n",
    "        res = read_pkl(f'/public/home/hpc244706074/compared_method_results/{n}_{ont}_res_proposed.pkl')\n",
    "        res=np.delete(res, del_id, axis=0)\n",
    "        compute_performance_test(go, res, s_plabels, ont)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ont in ['mf', 'cc']:\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    data = read_pkl('/public/home/hpc244706074/myProject/dataset/train_data_separate.pkl')\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/train_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    all_annotations=[]\n",
    "    for p in data:\n",
    "        item_set=set()\n",
    "        for item in data[p]['annotations']:\n",
    "            item=item.split('|')[0]\n",
    "            if go.has_term(item):\n",
    "                item_set |= go.get_anchestors(item)\n",
    "        all_annotations.append(list(item_set))\n",
    "    go.calculate_ic(all_annotations)\n",
    "    p_gos = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels.pkl')\n",
    "    s_plabels = {}\n",
    "    # for i,p in enumerate(p_gos):\n",
    "    #     if p != 'ATPF1_MOUSE':\n",
    "    #         s_plabels[p] = p_gos[p]\n",
    "    #     else:\n",
    "    #         del_id = i\n",
    "\n",
    "    res = read_pkl(f'/public/home/hpc244706074/TALE/output4/{ont}_tale+_predict.pkl')\n",
    "    pred_score = []\n",
    "    for i, p in enumerate(res):\n",
    "        if p != 'ATPF1_MOUSE':\n",
    "            pred_score.append(res[p])\n",
    "            s_plabels[p] = p_gos[p]\n",
    "    print(len(pred_score))\n",
    "    print(len(s_plabels))\n",
    "    compute_performance_test(go, pred_score, s_plabels, ont)\n",
    "    # pred_score = res\n",
    "    # F_max, Aupr, threadhold = compute_performance_test(proteins, pred_score, p_labels , ont)\n",
    "    # print('mf: aupr:%0.6f,F_max:%.6f,threadhold:%.6f\\n' % (\n",
    "    #                 (Aupr, F_max, threadhold)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tale+\n",
      "Have done mf F_max: 0.5726942804197396 Aupr: 0.5117563695440779 threadhold: 0.46 Smin: 8.266034732365059 ICAupr: 0.4831575790751277 DPAupr: 0.4486587324699323\n",
      "tale\n",
      "Have done mf F_max: 0.446267188954328 Aupr: 0.36500065608699 threadhold: 0.54 Smin: 10.235016912307188 ICAupr: 0.3343485630744795 DPAupr: 0.2964677980830466\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ont in ['mf']:\n",
    "    go_file = '/public/home/hpc244706074/myProject/dataset/go.obo'\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    data = read_pkl('/public/home/hpc244706074/myProject/dataset/train_data_separate.pkl')\n",
    "    proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/train_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "    all_annotations=[]\n",
    "    for p in data:\n",
    "        item_set=set()\n",
    "        for item in data[p]['annotations']:\n",
    "            item=item.split('|')[0]\n",
    "            if go.has_term(item):\n",
    "                item_set |= go.get_anchestors(item)\n",
    "        all_annotations.append(list(item_set))\n",
    "    go.calculate_ic(all_annotations)\n",
    "    p_gos = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/test_data_separate_{ont}_labels.pkl')\n",
    "    \n",
    "    terms = read_pkl(f'/public/home/hpc244706074/TALE/data/{ont}_go_1.pickle')\n",
    "    # print(len(terms))\n",
    "    tale_go_idx = {terms[g]['index'] : g for g in terms}\n",
    "    file_path = f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv'\n",
    "    all_go = list(pd.read_csv(file_path)['functions'])\n",
    "    # print(len(all_go))\n",
    "    true_idx = {g: i for i,g in enumerate(all_go)}\n",
    "    for m in ['tale+','tale']:\n",
    "        print(m)\n",
    "        res = read_pkl(f'/public/home/hpc244706074/TALE/output4/{ont}_{m}_predict.pkl')\n",
    "        pred_score = []\n",
    "        s_plabels = {}\n",
    "        for i, p in enumerate(res):\n",
    "            temp_res = [0]*len(all_go)\n",
    "            if p != 'ATPF1_MOUSE':\n",
    "                for j, r in enumerate(res[p]):\n",
    "                    temp_res[true_idx[tale_go_idx[j]]] = r\n",
    "                # temp_res = [res[p][i] for g]\n",
    "                pred_score.append(temp_res)\n",
    "                s_plabels[p] = p_gos[p]\n",
    "        # print(len(pred_score))\n",
    "        # print(len(s_plabels))\n",
    "        compute_performance_test(go, pred_score, s_plabels, ont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
