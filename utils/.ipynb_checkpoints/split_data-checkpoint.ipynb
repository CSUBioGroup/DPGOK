{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1cc9c42-a6d9-4246-84dd-205f909e2037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "from collections import deque, Counter,defaultdict\n",
    "\n",
    "# import argparse\n",
    "# def parse_args():\n",
    "#     parser = argparse.ArgumentParser(description='uniprot_API')\n",
    "#     parser.add_argument('--uniprot_sport_file1', default='./data/uniprot_sprot.dat')\n",
    "#     parser.add_argument('--uniprot_sport_file2', default='./data/uniprot_sprot.dat')\n",
    "#     parser.add_argument('--uniprot_sport_file3', default='./data/uniprot_sprot.dat')\n",
    "#     parser.add_argument('--output_file', default='./uniprot.pkl')\n",
    "#     parser.add_argument('--go_file', default='./data/go.obo')\n",
    "#     parser.add_argument('--bp_freq', default=50)\n",
    "#     parser.add_argument('--cc_freq', default=50)\n",
    "#     parser.add_argument('--mf_freq', default=50)\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "#     return args\n",
    "\n",
    "def read_pkl(input_file):\n",
    "    with open(input_file,'rb') as fr:\n",
    "        temp_result = pkl.load(fr)\n",
    "    \n",
    "    return temp_result\n",
    "\n",
    "def save_pkl(output_file,data):\n",
    "    with open(output_file,'wb') as fw:\n",
    "        pkl.dump(data,fw)\n",
    "        \n",
    "def get_label(anations,func_list):\n",
    "    temp_result = []\n",
    "    for label in func_list:\n",
    "        if label in anations:\n",
    "            temp_result.append(1)\n",
    "        else:\n",
    "            temp_result.append(0)\n",
    "    return np.array(temp_result)\n",
    "\n",
    "NAMESPACES = {\n",
    "    'cc': 'cellular_component',\n",
    "    'mf': 'molecular_function',\n",
    "    'bp': 'biological_process'\n",
    "}\n",
    "\n",
    "NAMESPACES_reverse = {\n",
    "     'cellular_component':'cc',\n",
    "     'molecular_function': 'mf',\n",
    "     'biological_process':'bp',\n",
    "}\n",
    "\n",
    "#实验类型数据\n",
    "EXP_CODES = set([\n",
    "    'EXP', 'IDA', 'IPI', 'IMP', 'IGI', 'IEP', 'TAS', 'IC',])\n",
    "#    'HTP', 'HDA', 'HMP', 'HGI', 'HEP'])\n",
    "\n",
    "#CAFA中的物种\n",
    "CAFA_TARGETS = set([\n",
    "    '10090', '223283', '273057', '559292', '85962',\n",
    "    '10116',  '224308', '284812', '7227', '9606',\n",
    "    '160488', '237561', '321314', '7955', '99287',\n",
    "    '170187', '243232', '3702', '83333', '208963',\n",
    "    '243273', '44689', '8355'])\n",
    "\n",
    "taxonTable = {'10116':'RAT','9606':'HUMAN','3702':'ARATH','7955':'DANRE','44689':'DICDI',\n",
    "    '7227':'DROME','83333':'ECOLI','10090':'MOUSE','208963':'PSEAE',\n",
    "    '237561':'CANAX','559292':'YEAST','284812':'SCHPO','8355':'XENLA','224308':'BACSU',\n",
    "    '99287':'SALTY','243232':'METJA','321314':'SALCH','160488':'PSEPK','223283':'PSESM',\n",
    "    '85962':'HELPY','243273':'MYCGE','170187':'STRPN','273057':'SULSO','all':'all','prokarya':'prokarya','eukarya':'eukarya'}\n",
    "\n",
    "\n",
    "def is_cafa_target(org):\n",
    "    return org in CAFA_TARGETS\n",
    "\n",
    "def is_exp_code(code):\n",
    "    return code in EXP_CODES\n",
    "\n",
    "def load_uniport_data(uniprot_file, max_seqlen):\n",
    "    proteins = list()\n",
    "    accessions = list()\n",
    "    sequences = list()\n",
    "    annotations = list()\n",
    "    interpros = list()\n",
    "    orgs = list()\n",
    "    \n",
    "    with open(uniprot_file, 'r') as f:\n",
    "        prot_id = ''\n",
    "        prot_ac = ''\n",
    "        seq = ''\n",
    "        org = ''\n",
    "        annots = list()\n",
    "        ipros = list()\n",
    "        \n",
    "        for idx, line in enumerate(f):\n",
    "            items = line.strip().split('   ')\n",
    "#             items = line.strip().split('\\t')\n",
    "            if items[0] == 'ID' and len(items) > 1:\n",
    "                if prot_id != '':\n",
    "                    proteins.append(prot_id)\n",
    "                    accessions.append(prot_ac)\n",
    "                    sequences.append(seq)\n",
    "                    annotations.append(annots)\n",
    "                    interpros.append(ipros)\n",
    "                    orgs.append(org)\n",
    "                prot_id = items[1]\n",
    "                annots = list()\n",
    "                ipros = list()\n",
    "                seq = ''\n",
    "            elif items[0] == 'AC' and len(items) > 1:\n",
    "                prot_ac = items[1]\n",
    "            elif items[0] == 'OX' and len(items) > 1:\n",
    "                if items[1].startswith('NCBI_TaxID='):\n",
    "                    org = items[1][11:]\n",
    "                    end = org.find(' ')\n",
    "                    org = org[:end]\n",
    "                else:\n",
    "                    org = ''\n",
    "            elif items[0] == 'DR' and len(items) > 1:\n",
    "                items = items[1].split('; ')\n",
    "                if items[0] == 'GO':\n",
    "                    go_id = items[1]\n",
    "                    code = items[3].split(':')[0]\n",
    "                    annots.append(go_id + '|' + code)\n",
    "                if items[0] == 'InterPro':\n",
    "                    ipro_id = items[1]\n",
    "                    ipros.append(ipro_id)\n",
    "            elif items[0] == 'SQ':\n",
    "                seq = next(f).strip().replace(' ', '')\n",
    "                while True:\n",
    "                    sq = next(f).strip().replace(' ', '')\n",
    "                    if sq == '//':\n",
    "                        break\n",
    "                    else:\n",
    "                        seq += sq\n",
    "        if len(seq) <= max_seqlen:\n",
    "            proteins.append(prot_id)\n",
    "            accessions.append(prot_ac)\n",
    "            sequences.append(seq)\n",
    "            annotations.append(annots)\n",
    "            interpros.append(ipros)\n",
    "            orgs.append(org)\n",
    "        else:\n",
    "            print(len(seq))\n",
    "            \n",
    "    return proteins, accessions, sequences, annotations, interpros, orgs\n",
    "\n",
    "class Ontology(object):\n",
    "    def __init__(self, filename='./uniprot_sprot_train_test_data_oral/go.obo', with_rels=False):\n",
    "        self.ont = self.load(filename, with_rels)\n",
    "        self.ic = None\n",
    "\n",
    "    def has_term(self, term_id):\n",
    "        return term_id in self.ont\n",
    "\n",
    "    def calculate_ic(self, annots):\n",
    "        cnt = Counter()\n",
    "        for x in annots:\n",
    "            cnt.update(x)\n",
    "        self.ic = {}\n",
    "        for go_id, n in cnt.items():\n",
    "            parents = self.get_parents(go_id)\n",
    "            if len(parents) == 0:\n",
    "                min_n = n\n",
    "            else:\n",
    "                min_n = min([cnt[x] for x in parents])\n",
    "            self.ic[go_id] = math.log(min_n / n, 2)\n",
    "    \n",
    "    def get_ic(self, go_id):\n",
    "        if self.ic is None:\n",
    "            raise Exception('Not yet calculated')\n",
    "        if go_id not in self.ic:\n",
    "            return 0.0\n",
    "        return self.ic[go_id]\n",
    "\n",
    "#     def load(self, filename, with_rels):\n",
    "#         ont = dict()\n",
    "#         obj = None\n",
    "#         with open(filename, 'r') as f:\n",
    "#             for line in f:\n",
    "#                 line = line.strip()\n",
    "#                 if not line:\n",
    "#                     continue\n",
    "#                 if line == '[Term]':\n",
    "#                     if obj is not None:\n",
    "#                         ont[obj['id']] = obj\n",
    "#                     obj = dict()\n",
    "#                     obj['is_a'] = list()\n",
    "#                     obj['part_of'] = list()\n",
    "#                     obj['regulates'] = list()\n",
    "#                     obj['alt_ids'] = list()\n",
    "#                     obj['is_obsolete'] = False\n",
    "#                     continue\n",
    "#                 elif line == '[Typedef]':\n",
    "#                     obj = None\n",
    "#                 else:\n",
    "#                     if obj is None:\n",
    "#                         continue\n",
    "#                     l = line.split(\": \")\n",
    "#                     if l[0] == 'id':\n",
    "#                         obj['id'] = l[1]\n",
    "#                     elif l[0] == 'alt_id':\n",
    "#                         obj['alt_ids'].append(l[1])\n",
    "#                     elif l[0] == 'namespace':\n",
    "                    #     obj['namespace'] = l[1]\n",
    "                    # elif l[0] == 'is_a':\n",
    "                    #     obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    # elif with_rels and l[0] == 'relationship':\n",
    "                    #     it = l[1].split()\n",
    "                    #     # add all types of relationships\n",
    "                    #     if it[0] == 'part_of':\n",
    "                    #         obj['is_a'].append(it[1])\n",
    "                            \n",
    "#                     elif l[0] == 'name':\n",
    "#                         obj['name'] = l[1]\n",
    "#                     elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "#                         obj['is_obsolete'] = True\n",
    "#         if obj is not None:\n",
    "#             ont[obj['id']] = obj\n",
    "#         for term_id in list(ont.keys()):\n",
    "#             for t_id in ont[term_id]['alt_ids']:\n",
    "#                 ont[t_id] = ont[term_id]\n",
    "#             if ont[term_id]['is_obsolete']:\n",
    "#                 del ont[term_id]\n",
    "#         for term_id, val in ont.items():\n",
    "#             if 'children' not in val:\n",
    "#                 val['children'] = set()\n",
    "#             for p_id in val['is_a']:\n",
    "#                 if p_id in ont:\n",
    "#                     if 'children' not in ont[p_id]:\n",
    "#                         ont[p_id]['children'] = set()\n",
    "#                     ont[p_id]['children'].add(term_id)\n",
    "#         return ont\n",
    "\n",
    "    def load(self, filename, with_rels):\n",
    "        ont = dict()\n",
    "        obj = None\n",
    "        with open(filename, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line == '[Term]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = dict()\n",
    "                    obj['is_a'] = list()\n",
    "                    obj['part_of'] = list()\n",
    "                    obj['regulates'] = list()\n",
    "                    obj['positively_regulates'] = list()\n",
    "                    obj['negatively_regulates'] = list()\n",
    "                    obj['alt_ids'] = list()\n",
    "                    obj['is_obsolete'] = False\n",
    "                    continue\n",
    "                elif line == '[Typedef]':\n",
    "                    if obj is not None:\n",
    "                        ont[obj['id']] = obj\n",
    "                    obj = None\n",
    "                else:\n",
    "                    if obj is None:\n",
    "                        continue\n",
    "                    l = line.split(\": \")\n",
    "                    if l[0] == 'id':\n",
    "                        obj['id'] = l[1]\n",
    "                    elif l[0] == 'alt_id':\n",
    "                        obj['alt_ids'].append(l[1])\n",
    "                    elif l[0] == 'namespace':\n",
    "                        obj['namespace'] = l[1]\n",
    "                    elif l[0] == 'is_a':\n",
    "                        obj['is_a'].append(l[1].split(' ! ')[0])\n",
    "                    elif with_rels and l[0] == 'relationship':\n",
    "                        it = l[1].split()\n",
    "                        # add all types of relationships\n",
    "                        if it[0] == 'part_of':\n",
    "                            obj['part_of'].append(it[1])  #---------------changed\n",
    "                            # obj['is_a'].append(it[1])\n",
    "                        elif it[0] == 'regulates':\n",
    "                            obj['regulates'].append(it[1])\n",
    "                        elif it[0] == 'positively_regulates':\n",
    "                            obj['positively_regulates'].append(it[1])\n",
    "                        elif it[0] == 'negatively_regulates':\n",
    "                            obj['negatively_regulates'].append(it[1])\n",
    "                    elif l[0] == 'name':\n",
    "                        obj['name'] = l[1]\n",
    "                    elif l[0] == 'is_obsolete' and l[1] == 'true':\n",
    "                        obj['is_obsolete'] = True\n",
    "        if obj is not None:\n",
    "            ont[obj['id']] = obj\n",
    "        for term_id in list(ont.keys()):\n",
    "            for t_id in ont[term_id]['alt_ids']:\n",
    "                ont[t_id] = ont[term_id]\n",
    "            if ont[term_id]['is_obsolete']:\n",
    "                del ont[term_id]\n",
    "        for term_id, val in ont.items():\n",
    "            if 'children' not in val:\n",
    "                val['children'] = set()\n",
    "            for p_id in val['is_a']:\n",
    "                if p_id in ont:\n",
    "                    if 'children' not in ont[p_id]:\n",
    "                        ont[p_id]['children'] = set()\n",
    "                    ont[p_id]['children'].add(term_id)\n",
    "        return ont\n",
    "\n",
    "\n",
    "    def get_anchestors(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while(len(q) > 0):\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for parent_id in self.ont[t_id]['is_a']:\n",
    "                    if parent_id in self.ont:\n",
    "                        q.append(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_parents(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        for parent_id in self.ont[term_id]['is_a']:\n",
    "            if parent_id in self.ont:\n",
    "                term_set.add(parent_id)\n",
    "        return term_set\n",
    "\n",
    "\n",
    "    def get_namespace_terms(self, namespace):\n",
    "        terms = set()\n",
    "        for go_id, obj in self.ont.items():\n",
    "            if obj['namespace'] == namespace:\n",
    "                terms.add(go_id)\n",
    "        return terms\n",
    "\n",
    "    def get_namespace(self, term_id):\n",
    "        if term_id in self.ont:\n",
    "            return self.ont[term_id]['namespace']\n",
    "        else:\n",
    "            return 'can not find'\n",
    "    \n",
    "    def get_term_set(self, term_id):\n",
    "        if term_id not in self.ont:\n",
    "            return set()\n",
    "        term_set = set()\n",
    "        q = deque()\n",
    "        q.append(term_id)\n",
    "        while len(q) > 0:\n",
    "            t_id = q.popleft()\n",
    "            if t_id not in term_set:\n",
    "                term_set.add(t_id)\n",
    "                for ch_id in self.ont[t_id]['children']:\n",
    "                    q.append(ch_id)\n",
    "        return term_set\n",
    "    \n",
    "def get_uniport_mess(go_file, input_file, filter_exp, cafa_targets, anchestor_annots, max_seqlen):\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    proteins, accessions, sequences, annotations, interpros, orgs = load_uniport_data(input_file, max_seqlen)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'proteins': proteins,\n",
    "        'accessions': accessions,\n",
    "        'sequences': sequences,\n",
    "        'annotations': annotations,\n",
    "        'interpros': interpros,\n",
    "        'orgs': orgs\n",
    "    })\n",
    "    \n",
    "    if filter_exp:\n",
    "        index = []\n",
    "        annotations = []\n",
    "        for i, row in enumerate(df.itertuples()):\n",
    "            annots = []\n",
    "            for annot in row.annotations:\n",
    "                go_id, code = annot.split('|')\n",
    "                if is_exp_code(code):\n",
    "                    annots.append(annot)\n",
    "\n",
    "            # Ignore proteins without experimental annotations\n",
    "            if len(annots) == 0:\n",
    "                continue\n",
    "            index.append(i)\n",
    "            annotations.append(annots)\n",
    "        df = df.iloc[index]\n",
    "        df = df.reset_index()\n",
    "        df['annotations'] = annotations\n",
    "    \n",
    "    if cafa_targets:\n",
    "        index = []\n",
    "        for i, row in enumerate(df.itertuples()):\n",
    "            if is_cafa_target(row.orgs):\n",
    "                index.append(i)\n",
    "        df = df.iloc[index]\n",
    "        df = df.reset_index()\n",
    "    \n",
    "    if anchestor_annots:\n",
    "        prop_annotations = []\n",
    "        for i, row in df.iterrows():\n",
    "            annot_set = set()\n",
    "            annots = row['annotations']\n",
    "            for go_id in annots:\n",
    "                go_id = go_id.split('|')[0] # In case if it has code\n",
    "                annot_set |= go.get_anchestors(go_id)\n",
    "            annots = list(annot_set)\n",
    "            prop_annotations.append(annots)\n",
    "        df['annotations'] = prop_annotations\n",
    "        \n",
    "    return df\n",
    "\n",
    "# 5.1 生成预测的label信息  根据train蛋白质中出现功能频率划分label,此处bp定为50，mf定为40，cc定为30\n",
    "def get_train_labels(uniprot_df,min_count_fre,go_file,output_path):\n",
    "    label_df={}\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    cnt = Counter()\n",
    "    \n",
    "    for ii,rows in enumerate(uniprot_df.itertuples()):\n",
    "        annots = rows.annotations\n",
    "        \n",
    "        annots_set = set()\n",
    "        for go_id in annots:\n",
    "            go_id,ex_code = go_id.split('|')\n",
    "            annots_set |= go.get_anchestors(go_id)\n",
    "        \n",
    "        for go_id in annots_set:\n",
    "            cnt[go_id] += 1\n",
    "    \n",
    "    for tag in ['bp','cc','mf']:\n",
    "        res = []\n",
    "        min_count = min_count_fre[tag]\n",
    "        for key, val in cnt.items():\n",
    "            if val >= min_count and go.get_namespace(key) == NAMESPACES[tag]:\n",
    "                res.append(key)\n",
    " \n",
    "        df = pd.DataFrame({'functions':res})\n",
    "        df.to_csv('{0}/select_min_count_{1}_{2}_labels.csv'.format(output_path,min_count,tag))\n",
    "        label_df[tag]=df\n",
    "    return label_df\n",
    "\n",
    "def generate_valid_data(uniprot_df1,uniprot_df2):\n",
    "    data_one = uniprot_df1.set_index('proteins')\n",
    "    data_two = uniprot_df2\n",
    "    \n",
    "    selected_protein = {}\n",
    "    for protein in data_one.index:\n",
    "        selected_protein[protein] = []\n",
    "\n",
    "    count = 0\n",
    "    temp_count = 0\n",
    "    index = []\n",
    "    for i, row in enumerate(data_two.itertuples()):\n",
    "\n",
    "        protein = row.proteins\n",
    "    \n",
    "        if protein in selected_protein:\n",
    "            continue\n",
    "\n",
    "        temp_count += 1\n",
    "        count += 1\n",
    "        index.append(i)\n",
    "\n",
    "    data_two = data_two.iloc[index]\n",
    "    data_two = data_two.reset_index(drop=True)\n",
    "    return data_two\n",
    "    \n",
    "def generate_test_data(uniprot_df1,uniprot_df2,uniprot_df3):\n",
    "    data_one = uniprot_df1.set_index('proteins')\n",
    "    data_two = uniprot_df2.set_index('proteins')\n",
    "    data_three=uniprot_df3\n",
    "    \n",
    "    selected_protein_one = {}\n",
    "    for protein in data_one.index:\n",
    "        selected_protein_one[protein] = []\n",
    "        \n",
    "    selected_protein_two = {}\n",
    "    for protein in data_two.index:\n",
    "        selected_protein_two[protein] = []\n",
    "\n",
    "    count = 0\n",
    "    temp_count = 0\n",
    "    index = []\n",
    "    for i, row in enumerate(data_three.itertuples()):\n",
    "\n",
    "        protein = row.proteins\n",
    "    \n",
    "        if protein in selected_protein_one:\n",
    "            continue\n",
    "        if protein in selected_protein_two:\n",
    "            continue\n",
    "\n",
    "        temp_count += 1\n",
    "        count += 1\n",
    "        index.append(i)\n",
    "\n",
    "    data_three = data_three.iloc[index]\n",
    "    data_three = data_three.reset_index(drop=True)\n",
    "    return data_three\n",
    "\n",
    "# 5.2 根据之前生成的label信息，对train和test数据集里的蛋白质功能进行性划分,,所有功能寻找到所有的父节点\n",
    "def separate_protein_data(uniprot_df,go_file,output_file,min_count_frequency,label_df):\n",
    "    go = Ontology(go_file, with_rels=True)\n",
    "    \n",
    "    result = {}\n",
    "    all_labels = {}\n",
    "    for tag in ['bp','cc','mf']:\n",
    "        all_labels[tag] = list(label_df[tag]['functions'])\n",
    "        \n",
    "    for row in uniprot_df.itertuples():\n",
    "\n",
    "        protein = row.proteins\n",
    "        accessions = row.accessions\n",
    "        sequences = row.sequences\n",
    "        annotations = row.annotations\n",
    "        interpros = row.interpros\n",
    "        orgs = row.orgs\n",
    "\n",
    "        result[protein] = {}\n",
    "\n",
    "        result[protein]['accessions'] = accessions.split(';')\n",
    "        result[protein]['sequences'] = sequences\n",
    "        result[protein]['annotations'] = annotations\n",
    "        result[protein]['interpros'] = interpros\n",
    "        result[protein]['orgs'] = orgs\n",
    "\n",
    "        for tag in ['bp','cc','mf']:\n",
    "            result[protein]['selected_{0}'.format(tag)] = set()\n",
    "            result[protein]['all_{0}'.format(tag)] = set()\n",
    "\n",
    "        \n",
    "        for anno in annotations:\n",
    "            anno, ex_code = anno.split('|')\n",
    "            annots_set = go.get_anchestors(anno)\n",
    "            for anno2 in annots_set:\n",
    "                inner_tag = NAMESPACES_reverse[go.get_namespace(anno2)]\n",
    "                result[protein]['all_{0}'.format(inner_tag)].add(anno2)\n",
    "                if anno2 in all_labels[inner_tag]:\n",
    "                    result[protein]['selected_{0}'.format(inner_tag)].add(anno2)\n",
    "    print(len(result))\n",
    "    save_pkl(output_file, result)\n",
    "    \n",
    "def main(uniprot_sport_file1,uniprot_sport_file2,uniprot_sport_file3,output_path,go_file,min_count_frequency, max_seqlen):\n",
    "    filter_exp = True\n",
    "    cafa_targets = True\n",
    "    anchestor_annots = False\n",
    "    uniprot_df1=get_uniport_mess(go_file, uniprot_sport_file1, filter_exp, cafa_targets, anchestor_annots, max_seqlen)\n",
    "    uniprot_df2=get_uniport_mess(go_file, uniprot_sport_file2, filter_exp, cafa_targets, anchestor_annots, max_seqlen)\n",
    "    uniprot_df3=get_uniport_mess(go_file, uniprot_sport_file3, filter_exp, cafa_targets, anchestor_annots, max_seqlen)\n",
    "    valid_df=generate_valid_data(uniprot_df1,uniprot_df2)\n",
    "    test_df=generate_test_data(uniprot_df1,uniprot_df2,uniprot_df3)\n",
    "    label_df=get_train_labels(uniprot_df1, min_count_frequency, go_file,output_path)\n",
    "    separate_protein_data(uniprot_df1,go_file,output_path+'/train_data_separate.pkl',min_count_frequency,label_df)\n",
    "    separate_protein_data(valid_df,go_file,output_path+'/valid_data_separate.pkl',min_count_frequency,label_df)\n",
    "    separate_protein_data(test_df,go_file,output_path+'/test_data_separate.pkl',min_count_frequency,label_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10281f46-aad0-46e7-8ea6-a46803b84b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57409\n",
      "1058\n",
      "1787\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "uniprot_sport_file1='/public/home/hpc244706074/myProject/data_cache/uniprot_202201/uniprot_sprot.dat'\n",
    "uniprot_sport_file2='/public/home/hpc244706074/myProject/data_cache/uniprot_202301/uniprot_sprot.dat'\n",
    "uniprot_sport_file3='/public/home/hpc244706074/myProject/data_cache/uniprot_202405/uniprot_sprot.dat'\n",
    "output_path='/public/home/hpc244706074/myProject'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "if not os.path.exists(output_path+'/result_file/'):\n",
    "    os.mkdir(output_path+'/result_file/')\n",
    "output_path=output_path+'/result_file/'\n",
    "go_file='/public/home/hpc244706074/myProject/data_cache/go.obo'\n",
    "bp_freq=int(1)\n",
    "cc_freq=int(1)\n",
    "mf_freq=int(1)\n",
    "min_count_frequency={'bp':bp_freq,'cc':cc_freq,'mf':mf_freq}\n",
    "max_seqlen = 100000000\n",
    "main(uniprot_sport_file1,uniprot_sport_file2,uniprot_sport_file3,output_path,go_file,min_count_frequency, max_seqlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7259c8d5-7130-4756-b512-046d8dbdf4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data_separate bp 1266\n",
      "test_data_separate cc 1006\n",
      "test_data_separate mf 703\n",
      "valid_data_separate bp 703\n",
      "valid_data_separate cc 633\n",
      "valid_data_separate mf 422\n",
      "train_data_separate bp 46565\n",
      "train_data_separate cc 41539\n",
      "train_data_separate mf 33339\n"
     ]
    }
   ],
   "source": [
    "#文件夹train_test_data_handled_v3下的数据是整理好的蛋白质数据，根据pkl文件中的数据，生成bp、cc和mf对应的label信息\n",
    "min_count_frequency = {'bp':1,'mf':1,'cc':1}\n",
    "\n",
    "predict_func = {}    \n",
    "for tag in ['bp','cc','mf']:\n",
    "    predict_func[tag] = list(pd.read_csv(\"/public/home/hpc244706074/myProject/result_file/select_min_count_{1}_{0}_labels.csv\".format(tag,min_count_frequency[tag]))['functions'])\n",
    "        \n",
    "\n",
    "save_path = '/public/home/hpc244706074/myProject/result_file/'\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "for file in ['test_data_separate', 'valid_data_separate', 'train_data_separate']:\n",
    "    input_file = \"/public/home/hpc244706074/myProject/result_file/{0}.pkl\".format(file)\n",
    "    datas = read_pkl(input_file)\n",
    "    \n",
    "    for tag in ['bp','cc','mf']:\n",
    "        gos = {}\n",
    "        labels = {}\n",
    "        protein_sequence = {}\n",
    "        select_proteins = []\n",
    "        \n",
    "        for key,value in datas.items(): #key = protein\n",
    "            anations = value['selected_{0}'.format(tag)]\n",
    "            if len(anations) == 0:\n",
    "                continue\n",
    "            \n",
    "            # 获取蛋白质id列表和gos和标签list\n",
    "            select_proteins.append(key)\n",
    "            protein_sequence[key] = value['sequences']\n",
    "            gos[key] = len(value['all_{0}'.format(tag)]) - len(value['selected_{0}'.format(tag)])\n",
    "            labels[key] = get_label(anations,predict_func[tag])\n",
    "            \n",
    "        df = pd.DataFrame({'proteins':select_proteins})\n",
    "        df.to_csv(save_path+\"{0}_{1}_proteins.csv\".format(file, tag))\n",
    "        save_pkl(save_path+\"{0}_{1}_gos.pkl\".format(file, tag),gos)\n",
    "        save_pkl(save_path+\"{0}_{1}_labels.pkl\".format(file, tag),labels)\n",
    "        save_pkl(save_path+\"{0}_{1}_sequences.pkl\".format(file, tag),protein_sequence)\n",
    "        print(file, tag, len(select_proteins))\n",
    "        \n",
    "        \n",
    "# train 57409\n",
    "# valid 1058\n",
    "# test 1787\n",
    "\n",
    "# test_data_separate bp 1313\n",
    "# test_data_separate cc 1006\n",
    "# test_data_separate mf 703\n",
    "# valid_data_separate bp 731\n",
    "# valid_data_separate cc 633\n",
    "# valid_data_separate mf 422\n",
    "# train_data_separate bp 47140\n",
    "# train_data_separate cc 41539\n",
    "# train_data_separate mf 33339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d5fcd02-12e0-4879-b652-b64ad54bb273",
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in ['test','valid','train']:\n",
    "    data = read_pkl(f'/public/home/hpc244706074/myProject/dataset/{t}_data_separate.pkl')\n",
    "    for ont in ['mf', 'bp', 'cc']:\n",
    "        all_gos = {}\n",
    "        proteins = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/{ont}/{t}_data_separate_{ont}_proteins.csv')['proteins'])\n",
    "        for p in proteins:\n",
    "            all_gos[p] = data[p][f'all_{ont}']\n",
    "        save_pkl(f'/public/home/hpc244706074/myProject/dataset/{ont}/{t}_data_separate_{ont}_labels_goname.pkl',all_gos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081fd5c-b188-4c97-9886-1935c4055620",
   "metadata": {},
   "source": [
    "### 生成训练加测试集的GO标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0183407c-e6b0-4f07-95aa-f09c4f2d6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mf\n",
      "train gos: 6091\n",
      "train&test gos:6147\n",
      "bp\n",
      "train gos: 18832\n",
      "train&test gos:18906\n",
      "cc\n",
      "train gos: 2604\n",
      "train&test gos:2625\n"
     ]
    }
   ],
   "source": [
    "for ont in ['mf','bp','cc']:\n",
    "    print(ont)\n",
    "    train_gos = list(pd.read_csv(f'/public/home/hpc244706074/myProject/dataset/select_min_count_1_{ont}_labels.csv')['functions'])\n",
    "    print(f'train gos: {len(train_gos)}')\n",
    "    test_data = read_pkl('/public/home/hpc244706074/myProject/dataset/test_data_separate.pkl')\n",
    "    for p in test_data:\n",
    "        gos = test_data[p][f'all_{ont}']\n",
    "        for g in gos:\n",
    "            if g not in train_gos:\n",
    "                train_gos.append(g)\n",
    "    print(f'train&test gos:{len(train_gos)}')\n",
    "    df = pd.DataFrame({'functions':train_gos})\n",
    "    df.to_csv(f'/public/home/hpc244706074/myProject/dataset/train&test_{ont}_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea160d4f-06f6-4025-b5e0-9cc5c4c7e544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myproject",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
